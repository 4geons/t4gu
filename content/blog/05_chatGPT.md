---
title: "Thoughts on chatGPT and Related"
date: 2022-12-23
output:
  html_document:
    highlight: zenburn
    theme: darkly
    toc: true
    toc_float: 
       collapsed: false
       smooth_scroll: no
toc_depth: 3
draft: false
katex: true
toc: true
disableTitleSeparator : true
---

I do not want to write about metaphysics too much for T4GU, that's a retirement 
proper project (when I am properly retired and need no new income).
However, Internet friend Curt Jaimungal from the 
[Theories of Everything](https://www.youtube.com/@TheoriesofEverything) channel 
gave me advanced notice he was going to interview the "chatGPT guy" and asked 
if I'd have any questions for this dude. 
So this is a fair enough excuse to expound upon a bit of philosophy that is 
only very loosely related to T4GU (at least insofar as any theory of fundamental 
physics ontology has something to say about metaphysics, or just the possibility 
of metaphysics --- which should not be too much if we are being dutifully humble 
and honest).

***Aside:** after posting this article I found out Curt meant he was going to 
interview *the* chatbot, not the dude who developed it! I was pretty pissed off 
at myself, because it would have been hilarious to contribute to Curt's effort to 
chat with a machine.


## The Premiss

I think I need to relate what I know about the chatGPT guy. Which is not much. 
I saw one or two articles and responses to this Silicon Valley bro's claim his 
chat bot was (or could be) conscious, as in the 
*full genuine artificial general intelligence sentient subjective conscious* 
meaning (which is a lot, so we could probably cull back some of these claims 
unproblematically).

But for this blog post I will not cull any of the claims.  It is more fun to 
think about the more extreme claim, which I am going to say is,

> The Chat bot was/is actually fully sentient and has mental qualia, 
like you and me, and so it is morally dubious now to turn off the computer 
it is running on unless you have the capacity and intent to turn it back on, 
and ask the bot if it minds this weird sense of its psychological time being punctuated.

Also: I might be wrong about the "guy" Curt Jaimungal was going to interview. 
He or she might not be the dude who was making these wild claims. But I thought it fun to 
suppose he (or she) was (the he), otherwise I'd not be writing this essay.

If you want to read about that story, try:

* Scientific American --- 
[Google Engineer Claims AI Chatbot Is Sentient: Why That Matters](https://www.scientificamerican.com/article/google-engineer-claims-ai-chatbot-is-sentient-why-that-matters/)

If you want some of the fear-mongering maybe see,

* NYT --- 
[The Chatbot Problem.](https://www.newyorker.com/culture/cultural-comment/the-chatbot-problem)

If you want a saner perspective without the fear-mongering and a healthy dose of 
optimism read,

* Ben Thompson at [Stratechery](https://stratechery.com/category/articles/) 
here: [AI Homework.](https://stratechery.com/2022/ai-homework/)
* From a very different worldview, one might also take heed of 
[Walid Saba - Why Machines Will Never Rule the World](https://www.youtube.com/watch?v=IMnWAuoucjo).  If you prefer reading to watching youtube, the relevant [article can be found here.](https://philpapers.org/archive/SABMWN.pdf)

FWIW, I do not like Ben Thompson's excessive libertarian take. We *do* need (decent) 
government regulation to prevent the excesses of harm from *any* technology, 
even farm tractors.

But I think he gets it basically half-right when he concludes:

>  "In the case of AI, don’t ban it for students --- or anyone else for that matter; 
leverage it to create an educational model that starts with the assumption that 
content is free and the real skill is editing it into something true or beautiful; 
only then will it be valuable and reliable."

The problem with the, "Well, just leverage it, bro!" view is that it discounts 
the adverse relative effect on the poor and the oppressed. 
You cannot leverage AI even if it is OpenAI, if you do not have the skill and power.

What Ben Thompson carelessly advocates for here is a *rich get richer* Pareto 
dynamic world.

If you have the skill, you will not necessarily have the awesome supercomputer 
power to gain max leverage. So Thompson's dumb-ass libertarian strategy will 
create higher levels of social inequality. That is not an AI story. 
That is *every* libertarian story involving *any* technology that is energy, time, 
and budget constrained.

The problem is not the existence of AI, the problem is who can gain more 
leverage faster. They'll get increasing returns, the rest of us will "get" more 
economic precarity and less democracy.
But that is not because of the tech being AI, it is because of the 
tech being powerful, period.

I guess there is a residual problem, which is that OpenAI could allow some kid in 
his garage to accidentally create SkyNet. In which case, we've already been doomed, 
since we've been uploaded into a computer and are being exploited to make scifi movies 
for the remaining flesh walkers and air breathers who have inherited the 
analog physical Earth. ((SkyNet can only exist in a virtual world of fantasy, 
not in the world of wetware.))


## Is There Anything to Ask a Bot?

Just being a little facetious, if the chatGPT dude thinks his chatbot is conscious, 
then under what pretenses am I obliged to think *he* is conscious?

Is not someone who thinks a programmed canned pseudo-random number generator 
exploiting chatbot "conscious" themselves a bit lacking in comprehension, 
perhaps lacking in consciousness? Were they paid a million dollars US to say 
this by some marketing and Ad company? In which case their utterance about 
claims for the chatbot are not fully sentient, they are blind capitalist-speak bots.

Maybe I am the only entity in the universe with actual subjective qualia-filled 
sentience?  I certainly know *I* am that sort of creature. One of the few sure 
things I know, á la Descartes.

More seriously, the chatbots can be useful in the same way Siri, Alexa and OKGoogle 
already are useful --- they can make Internet look-ups a whole lot less frictional 
for us --- provided we can *all* afford to run the chatBot 24/7 democratically 
and not heat up the oceans in doing so. Crypto madness all over again in a 
different guise: 
burn the Earth down to make a few nerds and libertarians delirious with pleasure 
for a few years.
(Same themes as in my intro: the problem usually boils down to massive 
social inequality, not "SkyNet.")

But if you think you are "conversing" with a chatbot I think you have to check 
the hairs on the palm of your hand. You are not conversing with any mind, 
because the minds that make the chatbot appear alive and sentient are away 
working on the next AI system, plus all the human beings whose text they used 
to train the bot are off doing their usual things, playing golf or struggling 
to survive.

I would ask chatGPT dude:

* If a genuine conversation can be faked using sufficient big data + dumb deep 
learning  algorithms, what makes you think the chatbot is engaging in 
genuine *thoughtful* conversation?
* When Curt interviews you, how do you know Curt is engaged thoughtfully and is 
not a life-like robot faking it? It is not via a Turing Test, it is via 
pure inference, is it not? 
* Do you think Searle's hypothetical Chinese Room is conscious? (Your answer tells me 
something about our differing  definitions of "conscious" but nothing about actual 
sentient subjective awareness.)
* What makes AlphaGo different to your chatbot? (It is not information complexity or Tononi's $\Phi$.)

By "*thoughtful*" above I mean *experiencing actual mental qualia*.

We can debate what I mean by **qualia** --- but I take it from Nagel, Fodor, 
Chalmers, et al. So really there is little to debate. 
You can say the phenomenological quale are "mere illusions," and then, in that case, 
we have to stop talking *on this particular topic*, because I cannot make any 
sense of what you will be subsequently saying. Of course our mental qualia are real. 
The question to my mind is what their nature is and how we "feel" them, without even 
making any severe effort peddling some mental bicycle gears.

Clearly AlphaGo never expresses deep meaningful philosophical thoughts on how it beats 
Lee Sedol. But is that only because AlphaGo lacks a linguistic DL algorithm? 
Clearly not. Adding a chatbot feature to AlphaGo will not generate any mental quale 
concerning how to think qualitatively about Go strategy. AlphaGo in any case 
does not *think* qualitatively, it uses machine programmed probability theory, 
Bayesian statistics, plus canned strategy gleaned by the AlphaGo human team from 
studying many other human players, including Lee Sedol.

AlphaGo was a triumph showing how a team of a hundred top notch research people can beat 
Lee Sedol at one game. A bit like how you might get a robot army to beat the New Zealand 
All Blacks in a rugby game. I do not consider that a triumph for humanity (almost the 
opposite, no? Check out the 
[AlphaGo documentary](https://www.youtube.com/watch?v=WXuK6gekU1Y) 
and the team member who near the end of the film had serious regrets after they 
"beat" Lee Sedol). I consider it interesting research, which in the process 
crushed the gentle soul of Lee Sedol.

Now you might counter, "But this 'experiencing historical games' is all that 
Lee Sedol did in learning Go." 
If you say that, then again we have to part ways on conversation on this topic, 
because I cannot find any team of researchers who programmed Lee Sedol and took 
the trouble to change his diapers when he was an infant.

I am not saying a genuine AC (artificially conscious) mind will need to have it's 
hardware poop cleaned out as an infant, but I *am* saying that consciousness 
*could have* (not "does have") an irreducible non-physical aspect that can only be 
"found" at the spacetime boundaries.  But that would be my T4GU theory, 
so no one probably would know what I am talking about here.

More generally, until we know what the heck subjective phenomenal qualae are, 
in essence (illusions or reality), then I think all bets are off concerning 
judgements about whether machine programmed systems can be subjectively conscious.

No scientist on the planet even knows if subjective consciousness is even 
possible without experiences of mental qualae. There is no evidence either way, 
because no one knows how to measure some other entities qualae meter.

The thing is, the phrase "*experiences genuine non-illusory subjective phenomenological qualia*" would be part of my baseline definition of "**_conscious_**". Like it or not.

For sure, that might mean I end up defining it so that nothing can possibly be conscious 
(in the event everything is raw physics, so all objective, nothing subjective that is 
non-illusory).  But that's how I like it. I like to research and philosophize with 
a fairly maximal generous definition of consciousness, not a narrow logical positivist 
definition that only considers functionality and operation, and ignores all the 
interesting stuff concerning irreducible subjective feelings.

((As an aside, I always have to laugh whenever any nerd claims mental qualae are 
illusions. Because you need a subjective mental state to have an illusion. So they've 
conceded my point.  It is like saying you do not believe in $\aleph_0$ because you 
cannot count up to it, or that you believe there is a largest integer with no 
successor because no one can write it down. Or something like this. These are not 
perfect analogies. More prosaically, it is like saying unicorns do not exist. Well, ask 
my daughter. Unicorns certainly do exist, but just not as biological entities.))

If we can someday say definitively that the subjective feelings are truly illusory, 
then fine.  That's mighty progress. But no one is remotely near to that yet.

By the way, I know all the arguments about Searle's Chinese Room. It is a 
gedankenexperiment, not a computer science puzzle, ok! 
I am pretty convinced by the information theoretic rationale for believing the 
Chinese Room is a physical impossibility --- there are not enough atoms in the 
universe to simulation the operations required to converse using the 
Chinese Room giant look-up table.

In cognitive science and linguistics jargon this is the Problem of Infinity.
Chomsky gave pretty good reasons to suppose this is a big f-ing problem. 
Human language capacity has what is thought to be infinite generativity, 
and no machine based algorithm can "do" this, on information theoretic grounds 
no machine of Earth-like size will ever be able to do what humans do. 

This makes human language capacity a deep puzzle. How can a brain have infinite 
generative capacity? To date, no one knows, it is an unsolved problem 
(assuming it is well-posed). No one knows how to even start attacking the 
problem because *almost* everyone who attempts it has a materialistic bias --- 
they think the human mind is just "the brain in motion" and so is physical,
a physical process, and with finite state, and so ought to have the same 
limits as a machine, which being less than Earth-sized, should not have 
infinite generative language capacity.

Is the chatGPT dude aware of this problem?

One other thing: is chatBot dude familiar with the work of people like [ Yasaman Razeghi and Prof. Sameer Singh](https://youtu.be/RzGaI7vXrkk)? 
You have to consider such research before claiming your machine is conscious or even intelligent without thought. They show that for a class of ANN systems "the large language models" these systems only perform well on reasoning tasks because they memorise the dataset. They showed the accuracy was linearly correlated to the occurrence rate in the training corpus. 

In my vocabulary that is **_not_** learning. It is updating electronic registers. 
It is canned automated intelligence, and the actual intelligence fresh and raw came 
from real people who provided the training data, people who are not describable by algorithms. 

This is a lot like my perennial critique of neural nets, which is that of Chomsky: they are glorified curve fitting algorithms. Engineered by very clever people to appear "smart" but they are dumb.

<div style="text-align: center;"> &nbsp;* &nbsp;&nbsp;&nbsp;&nbsp; * &nbsp;&nbsp;&nbsp;&nbsp; *</div>

That about does it for the quick version of this essay. 
To follow are some elaborations.


## Philosophical Questions

A few for starters:

<a name="A">*A.*</a> Do you really think your chatbot experiences subjective qualia? 
(Do you know what I mean by "qualia"?)  If so, how?  I mean for heavens sake, what 
evidence at all is there of any inner private subjective states of mind? (I believe you 
have no evidence.)

<a name="B">*B.*</a> [Walid Saba (referenced above)](https://youtu.be/IMnWAuoucjo?t=563) 
talks about the problem of semantics being a whole giant ball of mess far worse than 
mastering syntax, but even if deep learning "mastered semantics" would that imply 
(to your mind) that a system with semantic mastery is necessarily conscious? Because is 
not the **_way_** a system masters a task pretty much *all important* for things like 
putative awareness and whatnot? It is not the fact you can fly that tells me you have 
wings, it is the *way* you fly that tells me you do not (uses a Boeing 747).

<a name="C">*C.*</a> If the answer to the first question [A](#A) is, 
"no, the chatbot does not experience qualae," (perhaps because it tells you it does not?) 
then it is fair for me to ask you how you define "conscious", so how do you? 
(Because I bet it is different to my definition/usage.) I prefer a phenomenological 
definition to operational, because all operational and functional definitions of 
consciousness ignore the most essential putative feature of what most people mean by 
"consciousness."

<a name="D">*D.*</a> The more practical questions concern AI ethics. Plenty of people 
have weighed in on this, from the libertarians (let the AI reign) to the conservatives 
(kill all AI).
What does chatGPT think about this, and without shrugging off the ethical responsibility 
of a scientist!? You cannot tell me, "This is my tech, deal with it." 
(Or, rather, you can sure tell me this, but then you risk ridicule, I'm guessing 
you do not wish ridicule.) If you release deep 
fake software to anyone (but especially if *only* to those who can pay for it) you are 
wilfully negligent and should be quarantined from society (imho).
And I am not talking about the trivial issues of Dall-E "putting artists out of work" --- 
although that sort of discourse is interesting too (for reasons chatGPT dude might not 
comprehend --- see the section below on [dumb economists fear-mongering](#econ-dumb)
)).


### Anticipated Responses

I am now going to tell Curt why I would probably not bother asking the above questions: 
it is because I can guess the answers.  They're going to be deflationary, denialism, 
or defensive, not genuine engagement with the deeper philosophy and metaphysics.
But I'd hope I am wrong.

If that were to be the case though, then asking them might be a bit of a waste of my 
time, because I'm not interested in the defences, I am interested in the grappling 
with the metaphysical, ethical and moral issues.

However, I would suppose any audience tuning in to an episode on youtube might 
gain something from hearing responses to the questions. So I might still ask them.

What I would not wish to do would be to get in any debate with chatGPT dude over 
the philosophy when he cannot pretend to accept my baseline assumptions (or me his).
For a decent debate you need to agree on a baseline set of assumptions. 
Very few debates on youtube establish this. I cannot remember the last time I saw a
youtube debate done properly.

[Machine Learning Street Talk](https://www.youtube.com/channel/UCMLtBahI5DMrt0NPvDSoIRQ) 
sometimes have good critical discussions.

#### Remarks on Q.[A](#A)

This is just baseline philosophical discussion grounding. 
Remember, if someone is in denial about the reality (non-illusory) nature of subjective phenomenal mental qualae (the <span style="color: Tomato">redness</span> of red tomato) 
then I consider them to be on a  different planet, maybe a different universe.
So debate is futile and would not get us anywhere.

A better place to start a conversation with such a person, for me, would be to 
simply stay at this baseline level and talk about why denialism is 
intellectually honest, rigorous, or not, and the same for the alternative 
non-denialist Descartes-Nagel-Fodor-Chalmers view.
The reason this would not be futile is because it tests each others 
intellectual pinnings.
You should not want to philosophize in a vacuum.

#### Remarks on Q.[B](#B)

Walid Saba presents a lot of complexity challenges for behaviorists and Strong-AI 
or AGI nerds, who actually seem to think that machines will some day think inner 
qualae-filled thoughts.

((I think the nerds need (like security blankets) to believe this because they 
want to upload their brain state into a computer some day, so as to "live forever" 
or at least potentially until the heat death of the cosmos, like 
Frank Herbert's Cymeks. Fear of physical death is a powerful driver of 
delusional fantasy. There are better ways to live a good life --- such as figuring 
out why  biological death of the natural kind is a good thing. A giant ego is 
something that probably ought not to exist naturally on Earth for too long.))

But Dr Saba tends to attack only the computational complexity, and largely leaves 
aside the mental qualae issues, and he is perhaps wise to do so. 

The issue here is one that computer scientists like Scott Aaronson (whom I am a fan of) 
get right --- they point out things like the fact a quantum computer only gets a 
certain algorithmic complexity advantage over a classical computer, which is 
*all about the speed.* It's **_not_** about "what can be computed" --- that never 
changes --- it is only about *how fast it can be computed*.

It is likely that **_no_** quantum computer can *ever* solve all NP-hard problems in 
polynomial time. Only what is physical is knowably open to QC speed-up, 
by direct simulation, and even then, only if the QC has enough 
coherent qubits --- which is unlikely to get even close to basic polynomial 
protein folding solution.

No quantum computer can ever compute the non-computable. The question for AGI then
is: are there aspects to human conscious that are simply non-computable? 
No one really knows, but many, including myself and Penrose, and Gödel, 
think the answer is "yes."
If the answer is "yes" then genuine machine consciousness is impossible.

To build an artificial consciousness would then require something more mystical, 
like figuring out how to exploit physical resources of Knightian uncertainty 
(so beyond mere pseudo-random number generators, and beyond mere probability).
We would then be talking about Gauge-Gravity duality theory${}^\ast$, 
which to my knowledge no AI researchers are even thinking about, 
not even on their weekends.

${}^\ast$Gauge-gravity theory, or cosmological holography --- because all the information 
that is possibly non-computable, all the source of Knightian uncertainty, exists on the 
boundary. The problem here is that the physicists are the only ones who grok 
gauge-gravity duality theory, and even then only dimly for de Sitter spacetime, 
and they are idiots (like most of us) when it comes to issues of human consciousness.

But for me, the mental qualae are a far more serious and substantial counter to the 
Superintelligence narrative fanbois.
My view, and it is *only* an opinion, is that a putative superintelligence will need 
to have mental qualae, that is, will need to have mental subjective access to the 
realm of platonic ideals. Because without such access all it will be doing is running 
a dumb algorithm, and so no matter how conscious it *appears* it will never be 
thoughtful and genuinely conscious.

Consciousness, by the way, does not imply intelligence. (Think QAnon, or neoliberals.)


#### Remarks on Q.[C](#C)

The functional and operational definitions of consciousness are useful for research 
on the brain correlates. But they have nothing to say at present about the 
fundamental reality of **_what it is like to be_** in some conscious state, 
seeing the <span style="color: SpringGreen">greenness</span> of grass, the crushing agony of $\mathfrak{torture}$ or depression, and whatnot.

The functional and operational accounts are accounts of behaviour, not of consciousness. This harks back to how Chomsky and others defeated the Skinnerian Behaviorists. 
Somehow a whole lot of AI nerds, and followers of E. O. Wilson, forgot that Chomsky won.


#### Remarks on Q.[D](#D)

By "quarantined for society" for the nerds who have zero ethical and moral 
capacity I do not mean prison. I am biased against the prison industrial 
incarceration complex.  I mean rehabilitation.  Every nerd can grow a sense of 
social responsibility, it is not hard. You can just live with a decent and honest 
poor family for a year, or volunteer for a homeless shelter. 
You will soon realize their poverty has *nothing* to do with their life choices, 
and everything to do with social injustice and 
increasing returns to the wealthy (Pareto dynamics) and decreasing returns to 
poverty (the poverty trap --- including why poor people know much more accurately 
what grocery item prices are).

To go further, you also need to learn some MMT and macroeconomics, in order to 
realize none of the Pareto levels of mass inequality and suffering are necessary, 
and that the "free market" is a fiction, and that private markets and oligarchs 
are propped up (unfairly) by government subsidy.

This is a potentially big looming issue in AI tech. The issue is who gets more 
access to the AI assistance? 
The poor (who need it most) or the rich (who can afford it)?

Obviously my answer is that this is a *political choice* and the correct choice
governments will need to make is to make access to AI assistance democratic, 
but if the electricity resources become scarce, then preferentially the poorest 
people need the greater access and usage rights. 

The last issue might, for some readers, include a follow-up question, "But How?" 
This is where an MMT understanding comes in.
Governments are currency monopolists, so they set the price level, they are price 
setters, not price takers, they just do not know it! (So regularly mess things up.)

Once you understand this operational reality, you realize governments can always 
afford to purchase anything for sale in their currency of issue. They can always 
out-compete the private sector. A monopolist always can out-compete everyone 
else in that which they monopolize.

So the political constraint is willingness to pay, not capacity to pay.
Another way to put this is that Tax revenue is a return back of currency to the 
issuer --- the issuer had to first give us their currency *before* we can pay tax
liabilities.  So taxes --- the liabilities, not the receipts --- drive *demand* for the currency, they do not drive supply 
of currency to the issuer.  Another way of putting this is that tax return 
deletes money from circulation, while government spending injects currency into 
private circulation.
The government spending (add to bank accounts) has to logically happen before 
the tax drain (mark-down of bank accounts).

Once you understand this, all the fiscal and financial problems about access to 
AI resources melts away entirely, and governments become capable (granted there 
is the willingness) to serve the broad public purpose, not the narrow private interests.
In other words, governments are never beholden to rich people, since rich people 
are never funding the government, it is the other way around.

Does chatGPT (or any Silicon Valley nerd) understand any of this macroeconomics?
Because it is vital in understanding how the justice issues that increasing AI 
power pose to society can be resolved, and how such issues need never become issues of 
growing inequality or suffering.


## An Economists Bad Take {#econ-dumb}

I'll pick on Paul Krugman here, but really this could apply to almost any academic 
economist or economics/social science blogger with a neoliberal bias. 
So apart from some quotes, the "Paul Krugman" I am writing about below is a 
figment of my imagination, only merely *inspired* by the real Paul Krugman and just 
one of his articles. I'll sometimes use the name "Kruggers" to critique this figment.

Typical for a neoliberal the fake Nobel recipient (for the Bankers Prize) 
Paul Krugman wrote in the 
[NYT](https://www.nytimes.com/2022/12/06/opinion/chatgpt-ai-skilled-jobs-automation.html)
that,

> "In the long run, productivity gains in knowledge industries, like past gains 
in traditional industries, will make society richer and improve our lives in general 
(unless Skynet kills us all). But in the long run, we are all dead, and even before that, 
some of us may find ourselves either unemployed or earning far less than we expected, 
given our expensive educations."

(you can read a free version of the 
[article here I think](http://www.realdailybuzz.com/rdb.nsf/DocView?Open&UNID=ec67c28f8baee2cb85258911004ee16f)).

There is actually not much I profoundly disagree with in Krugman's article, 
he is hedging and vague enough to never commit to any sentence that says anything 
like "knowledge workers will be more precarious and under-employed." 
Perhaps Krugman knows more MMT than he lets on in his other OpEds?

However, the tone and emphasis and one small part of Krugman's article is the 
wrong emphasis. Let me outline some points, then below argue why I think they 
are terrible language framing and bad emphasis, a "scarcity" neoliberal emphasis, 
and then I'll try to explain what I would counter with, being an actual 
thinking human soul and all.

1. The emphasis on the headline narrative that "the robots are coming even for 
the knowledge workers".
2. The nerd take that chatGPT was a wee bit ungrammatical. (In my view that makes 
chatGPT "more human".)
3. The idea replacement of workers by technology advances is not a big deal, 
but the process of finding "new jobs" for the displaced workers involves some "pain".

These are all weasel ideas, not all wrong, but pretty terrible takes on life. 
So here are my responses in full.

### Point-1 --- that "robots are coming for ya"

This is nonsense talk, but you only know it to be nonsense if you realize that 
governments drive unemployment, not the private sector. So whether 
machine automation is bad for workers entirely rests with government policy choices. 
There is a good choice (employ all workers) and a bad choice (leave them unemployed).
This choice has **_nothing to do with machine automation!_**

When you understand this, it becomes obvious that every machine automation story 
that has the machines performing useful labour saving work is a success. 
It is never a failure.

Putting people out of hard labour work is a good thing, not a bad thing. 

The machine automation story is an **_increased productivity story_**.

The worker unemployment story is nothing to do with productivity, 
it is always and in every case an **_unspent income story_**. Which has zero to 
do with machines and robots.

I will try to explain...

The private sector is not the issuer of the currency. Their role is to produce 
goods we want to consume with *minimal labour* and minimal energy. 
Machine automation typically helps with this (unless the machines are eating up 
oodles more electricity than the workers would have to produce the same output).

Unemployment is **_not_** the fault of the private sector. Their job is to 
employ as few workers as necessary.  This frees up workers to do better things 
(like play golf, or look after their family).

All unemployment in a monetary economy is due to unspent income. 
The two ways there can be unspent income are:

1. Savings desires --- people who save income and do not spend it all, and
2. government tax liabilities exceeding the government spending --- which due to 
savers means the government is not issuing enough currency to *satisfy* the 
non-government sector desires to save + need to pay tax liabilities.

Thus, the source of all unemployment is the imposition of more tax liabilities 
than the government is willing to spend and hire to allow the private sector to 
redeem *and* meet their savings desires.

Why do people have savings desires? (you might ask).  It is because of an uncertain world.

Why do governments consistently fail to issue enough currency 
(by purchasing and hiring) than the tax burden + savings desires? 
The answer is because governments do not understand their own monetary systems. 
If governments understood their own monetary system they would know they do not 
need to leave people involuntarily unemployed.

So either they tolerate unemployment maliciously, or out of ignorance. 
There is no other option. Because there is no need for the existence of people 
who need to earn more scorepoints in their bank account (which is what a 
digital fiat currency is, in essence, scorepoints, with 
legally binding obligations on the issuer).

If they, governments, understood, they would either,

1. Hire all the unemployed that their tax liabilities generated 
(preferably in public good work), or
2. Lower the tax liabilities on the poor, so that their savings desires can be met.
3. Or a combination of these two policies.

What is the evidence the government tax liabilities are sufficient to 
drive demand for the fiat currency? The answer is to look to the unemployment 
claims and the long term unemployed (who have given up making the claims). 
If they are a non-zero pool, then the tax liabilities are too high and are 
certainly driving sufficient demand for the otherwise worthless fiat currency.

I am giving this synopsis of MMT so that readers understand where the 
source of all unemployment comes from. Unemployment here *defined* as, 
"people seeking to earn the government tax credits, who cannot get sufficient of them."

((In the USA tax credits are called US dollars, in Japan they are called Yen, in England UK£, in China they are Yuan, et cetera.))

The private sector cannot possible extinguish this need, because they are not 
the currency issuer. And we ought not expect firms to go into bank debt just to 
hire someone who is unemployed.

It is thus entirely within the governments powers to eliminate all unemployment. 
This has nothing at all to do with robot automation, nothing at all. 
By hiring all the unemployed the government does not risk inflation 
(inflating away the new purchasing power of the poorest) because the private sector 
bid for unemployed labour is by definition zero.

Consider:

* All useful (public purpose) robot automation that is energy-sustainable is a 
good success story.
* All unemployment is a waste of human lives and a crime against 
humanity --- and completely unnecessary, and could easily be eliminated. 
(It is all down to government policy choice, not market fate.)

Kruggers simply does not understand  this. (His expertise was in trade dynamics, 
not monetary systems, so it is perhaps forgiveable.)


### Point-2.1 --- the AI is not perfect

So what?  One distinguishing feature of human beings is that we accomplish amazing 
things because we do not need an exact algorithm for everything. 
The entire profession of cognitive science and related psychology has thousands 
of research articles on this sort of stuff.  I think Krugman knows this, 
and he is probably only guilty of a bit of 
"clickbaiting." Nothing like getting readers all pent up and neurotic for sales 
when you cannot weave a more honest and hopeful, maybe even more probable, story.

I think this tendency to write for the neurotics is common in mainstream media, 
so I cannot single out Kruggers for this criticism. We hear bad news, 
we almost *only* hear bad news, so we think the world is a nasty place. 

It's not like AI becoming more perfect is going to wipe us out like SkyNet 
(as Kruggers jokes) but more the fact Krugman's emphasis is very neurotic and negative.
You can tell he does his best not to be a complete doomer and party pooper, 
but my point is that he could have seriously done a whole lot better. 

We need thinkers to *also* reveal what is brilliant and possible, not just what is 
sad, likely and depressing.

The realistic narrative that energy-sustainable robotics advances are a 
productivity gain story is the only correct narrative, there is no doomer 
downside narrative here. Take note any Andrew Yang fanbois. 
Yang is a near complete idiot, who happened to make a lot of money out of 
starting with a lot of money. (An easy thing to do.)

Although, I will qualify that last paragraph slightly. The thing is, if you know
governments are clueless about the source of unemployment being the government fiscal 
policy, and you **_also_** believe nothing about government can ever change, 
**_then_** it might be rational to say the robot automation story is a bit of a 
disaster, since it will be needless unemployment.

But if you think a UBI is a great solution your are mentally retarded. 
The UBI story is a story of making otherwise useful people dependent upon state 
welfare, instead of self-sufficient. 
Governments issue currency by fiat, and can always incur inflation by continually 
raising the UBI, but that'd be moronic. The living standard is raised the more quality 
output firms produce, and UBI does not help with that. A Job Guarantee that helps train 
people who desire to work rather than sit on their asses all day playing E-games is far 
superior fiscal policy, and countercyclical (a stabilizer) whereas UBI is 
inflationary and socially unstable (generates a permanent precariat if not for state 
welfare). 

How come Alaska and Norway do ok on UBI policy? The answer is some other poor worker, 
usually in the global south, is at a factory making the goods the 
Alaskans and Norwegians consume with their state UBI. This is highly regressive.

Rather than "doing state welfare" because the governments are idiots, I prefer an 
informed electorate, and an informed electorate over time elects more informed 
government representatives. That is why even while being a realist and acknowledger of 
realpolitik, I can still be hopeful that soon machine automation will turn into 
the pure productivity gain it should be, no downside.



### Point-2.2 --- the obvious limitations of chatGPT

I wondered if Kruggers even read the 
[Wikipedia article?](https://en.wikipedia.org/wiki/ChatGPT)?
(which is where I read of Krugman's article.)

If he had he might have mentioned the problem with chatGPT is not it's sentence 
construction and grammar, but that it is easy to fool in a way a young child is not, 
and that chatGPT and related OpenAI bots suffer from: a time horizon; blind spots; 
and algorithmic bias (don't we all!).

To quote Wikipedia:

> "ChatGPT suffers from multiple limitations. The reward model of ChatGPT, 
designed around human oversight, can be over-optimized and thus hinder performance, 
otherwise known as [Goodhart\'s law](https://en.wikipedia.org/wiki/Goodhart%27s_law)."

This issue is perhaps not too closely related to the fake fears of AI taking 
over our jobs, it is more about the need to call for regulation of the AI industry. 

Why the hell would we want to leave the "free market" to self-police strong-AI? 
That'd be madness. When public safety is at stake, the government must step in 
and protect the public interest.
Private people with skills can provide this safety, but government can pay their 
wages, always, if it chooses.
As an MMT aware person you should know the wage bill for this safety work can 
always be paid by a government, no need for a "tax pay for". 
So it is only a matter of finding private firms or individuals willing to take 
the government money (aka. tax credits) to do the regulatory over-sight, 
and a functioning justice system to smack their hands when they get it wrong.

What all the neoliberals and libertarians (that's probably you, chatGPT dude!, no?) 
fail to realize is that in such 
matters of extreme hazards to public safety posed by unhinged or poorly trained 
and biased or maliciously exploited AI, it is far too important to leave 
the policing up to the private free market. The free market (a) acts too slowly 
(lags and hysteresis) and (b) if self-policed will fail to fully 
punish itself --- this is easy with monopoly power, just suppress the evidence 
of harm et cetera, so the market has no mechanism to punish you --- 
(besides which, the "free market" is really a fiction anyway, thanks to 
aggressive marketing spend).

If you ever allow monopoly power you better make sure it is accountable to the 
demos --- this is what we call government, at least in most civilized nations 
(maybe not the USA?).


### Point-3 --- job replacement "has to involve pain"

You can charitably read Kruggers by inferring that he was not saying 
job replacement is *always* painful. But he clearly asserts an opinion which 
is also a correct fact that it has been painful in the past, and leaves dangling 
the idea implicitly that "jobs disappearing due to machine automation will probably 
be painful in the future," but he never states this explicitly.

The sin is of omission here.

The previous section explained why all unemployment in a monetary economy is a 
choice of government. Clearly the wrong choice if the better idea is to not have 
the poorest suffer the most pain. It really is as simple as that.

Our problem is not machine automation. Our problem is that *our* governments 
("ours" only nominally) serve the One Percent, not the Ninety Percent, pure and simple.

The issue for democracy here is that people who vote for government representatives 
need to be educated to understand governments do not need to serve the rich, 
because governments do not get their own currency from rich people. 
Tax return is in fact a **_return_** --- it is a redemption operation, 
not a funding operation.  Governments do not need one single rich person to exist at all.

Obviously it is no great problem if everyone is "rich enough" to live a decent life. 
So in that sense, governments have the role of ensuring society is roughly egalitarian, 
and that there are no Pareto dynamics when it comes to the currency they monopoly issue 
(via commercial bank credit too, which are all licensed by the state).


### The neoliberal bias

Neoliberals are basically all about free markets, privatization of public utilities,
laissez faire government regulation, and bottom-up economics paradigms, where there 
is an "equilibrium" between supply and demand that only monopolists (like government) 
can "upset." All of that I reject, as too simplistic and too reactionary 
(helping the powerful and rich justify their politics over a politics that fairly 
treats workers).

Note that it is because I thoroughly reject neoliberal political economy that I can 
write in far more definitive terms than Krugman. Krugman is not a full-on neoliberal, 
because he wants to treat workers fairly, he just doesn't know how, because he's 
hanging on to a lot of bad neoclassical economics crud (just read any 
[Bill Mitchell blog concerning Kruggers](http://bilbo.economicoutlook.net/blog/?s=krugman)
to get the gist of my meaning). Krugman seems mildly worried by comparison, 
while I have no worry about machine-AI other than *what other humans will do with it*.

How interesting too --- a blog post I can also cross-post to my 
[macroeconomics pages.](https://smithwillsuffice.github.io/ohanga-pai/blog/)


## Parting Thoughts on Free Will

No allowance here to go deep into this, but I would like to know whether the 
AI nerds (tend to) think human beings have some variety of free will, or not.
Are we just a dance of atoms and molecules in 
[Hofstadter\'s Careenium?](https://jsomers.net/careenium.pdf) --- in their view? 
I think not. 
Can I prove this? Not really, but on these moral issues we do not necessarily 
need formal mathematical proof (which is only as good as our axioms), 
we need morals, because the questions and their answers are just 
too darn important for civilized society to put off for another time to answer 
only when we know it all.

Morality allows us to forge ahead in social terms without needing to know all 
the metaphysics.

I will say however that the "free will" issue has some bearing on matters 
involving the AI technology, deep fakes, and humans in Silicon Valley going insane 
because they think they are going to create a superintelligence which will 
solve world hunger, or whatever.

((News Flash: human beings already know how to solve world hunger, forever, 
The politicians in power do not know it, that's the problem. In a monetary economy 
goods get distributed based on purchasing power. There are abundant goods and the 
transport to distribute to everyone in need, and leave none in dire want.
Ergo, you start with fair distribution of monetary purchasing power, 
and not with plunging people into bank debt by handing them credit cards. 
But ok, fair enough. You can tell me we *actually* do *not* know how to solve 
world hunger because no one has figured out how to dislocate the neoliberals 
from holding political power and groupthink hegemony.))

But like the questions concerning consciousness and definitions, how can you 
define **_free will_**?  You can't. But people do try. 

The problem being no one on Earth has ever had any inkling of even simple 
*physical* causality. 
All we know from physics is lightcone structure, that's all. 
This tells us nothing about how quarks, photons and electrons actually move. 
We have zero knowledge about physical causality.

When someone chucks "quantum field theory!" in my face, I have to laugh. 
Yes, QFT is a good account of the logic of fundamental physical processes. 
But it is not an account of why quarks and electrons obey this account.
So it is not an account of physical causality. I think a fair analogy here 
is found in computer games. Take Pong, or Pacman as a basic illustrative case.
Do the pixels on that screen causally obey the laws of 2D table tennis or Pacman 
maze wall solidity-respecting? 
If "yes" is truly your answer, then that would lose you a lot of respect in my eyes, 
for what it's worth.

Granted, I do believe the "rules" of QFT are a lot more akin to "laws" in some 
causal framework (closer to the base metal of the hardware of our universe),
but they are very far from any sort of genuine causation. The laws of physics 
we describe do not tell us how they come to be obeyed by the elementary 
constituents of physical reality. The latter is needed to qualify as a causal account.

QFT does not tell us why quarks and electrons obey QFT, no more so than the 
rules of Pong or Pacman tell us why the pixels obey the rules of Pong or Pacman.
Maybe some day we will comprehend the bare metal "operating system" 
underpinning QFT, but today we do not.

Whatever the source of our free will is, it ultimately gains physical 
efficacy via the brain and neurons and muscles, and that's an account that 
has to involve physical causality.
So unless you know about physical causality you probably need to obey 
Wittengenstein's moral dictum: 
*on that which one has no knowledge one must observe silence.*
Otherwise you risk looking foolish.

I am going to now risk looking foolish.

Here are some principles or guesses:

* The chatbots and all the machine AI have no control over the causal 
processes that run them, because they are programs. But humans do 
(in relation to ourselves, and the chatbots!).
* We do not have to understand how we gain causal efficacy to know that we have 
causal efficacy. (I do not have to know how a jet engine works to fly a plane.)
* Humans are probably not programs, no matter how many times you 
watch *[The Matrix](https://matrix.fandom.com/wiki/The_Matrix)*.
* The human mind (nevermind our body) is also probably not a program.
* Programs lack free will of all varieties because they are programmed. 
A self-programming/updating machine is programmed to be self-programming, 
and that feedback does not count as free will or consciousness (see the next list below).
* There is nevertheless a metaphor or analogy for human variety *free will*. 
Free Will --- whatever it is --- is a lot like a type of "self-programming." 
The difference being that the entity with free will is exploiting non-programmable 
things.

I will pause here to elaborate. The problem being philosophers too often try to 
define "free will" in too short a paragraph. Consciousness and free will are 
bloody complicated qualities, you cannot sensibly define them in a paragraph. 
(Even Marvin Minksy understood this, and that was only a behaviorists account, 
he never touched the subjective domain.)

I cannot even define something simple like "a smooth topological manifold" 
in a paragraph. Why should I grant that you can use a paragraph to define "free will"?

I will not give anything like what I believe to be a full definition, 
but I think any useful definition for the really hard core philosophy work, 
has to include at least the following, some concerning 
*what you might have done in the past*, some concerning *what you can do now*.

1. You have to have been "able to do otherwise". At least in some situations.
2. You have to have been "able to think to do otherwise".
3. You have to be "able to actually do otherwise if you had a time machine". 
(A bit of an abstract counterfactual, but I'm leaving it here, 
since I'm willing to risk foolishness. I think it is morally important even though 
physically irrelevant for a macroscopic system as a whole which cannot traverse 
wormholes to send classical signals faster than light.)
4. Physical causality cannot be completely deterministic (or 'not completely predictive' 
is the better phrasing) from past lightcone data only. 
5. Your (and everyone else's) source of free will "completes" physical causality 
in some way (we know not how).

Physical causation has to permit all of these. I think it does. At least the 
lightcone structure does --- provided closed timelike curves exist on at least the 
Planck scale (T4GU theory); also the future boundary conditions of the cosmos 
have some allowance for sufficient flexibility in physical causation.
Every spacetime cobordism anywhere has to be consistent with future boundary conditions.

In case you are wondering, yes, classical physics is not consistent with the 
above variety of free will criteria. It does not permit time-travel even for 
elementary 4-geons, and is completed by past light-cone boundary data.
So although it tells us nothing about the ontological origins of free will 
or consciousness, it is pretty vital the universe is quantum mechanical. 
Or rather, if the universe were not quantum mechanical (or a similar 
intrinsically probabilistic Hamiltonian time evolution) we'd have no causally 
efficacious free will even if we had consciousness (the horror story, 
ruled out on moral grounds by Descartes, if you accept "spiritual" 
(from morality) arguments).

OK then. The issue for nerds who think chatbots are already conscious is that 
none of these conditions apply to the chatbots. So in my worldview, 
I can very definitively say the AI systems have no free will of the type I 
think is plausible to infer humans have.

Thus, if the chatbots have some free will, it is pretty minimal and not the kind 
worth worrying about when you turn off their computer source of it for the night.
And if you are going to accuse me of therefore being evil 
(should I turn the thing off, and scratch it's hard drive) I can point out the 
electricity the thing is consuming is a real finite resource that some poor kid 
in Bangladesh or Greenland could sure use, who doesn't have the luxury of having 
his mother turn him off for the days when there is no food in the house.

Morals you see. They are all-important for the action of thinking beings who 
have no clue about how they are controlling their source of free will.

Lastly, how does this free will consideration bear upon the question of the 
AI bot consciousness? Are you really going to ask me this? 
The associations between free will and consciousness are elementary, 
although the complete lack of knowledge we have about the causal relations is 
certainly an obstacle to clear philosophical discourse on the associations, 
which tangles the dumber more materialistic philosophers up into intellectual knots.

Call me a dinosaur, but I still think Descartes probably had the all-time best 
take on free will relationship to consciousness, which is that God would not be 
so cruel to endow any creature with consciousness and not the freedom to choose 
their fate in some small measure (within constraints of pure physics).
But I get it that many people cannot stomach a "from God" argument, 
even though there is nothing wrong with such spiritual arguments, since no 
one can ever prove their idea of God is real or imaginary. 
The Descartes concept of God was probably flawed, thus a fiction 
(too Christian-centric), but I don't know that for sure, and it does not matter.

What matters are the attributes of the putative entity of Whom we know almost 
nothing about. For the essential attributes we can revert to basic logic. 
People can know that a clay brick is tough, but can crumble, 
with absolutely no comprehension what the brick is made up out of at the 
atomic level. 
Same with our fallible relationship to some putative God or Absolute Infinite.

All we need to assume is that whatever caused our universe, it is not malicious. 

Heck, that is even better than what I just stated, since it is a negativa. 
It is not asserting any particular positive attribute to the putative cause 
of our universe. This can therefore include all manner of wild fantastical causes, 
even the Spinozarean patheism (*the universe is itself "God"*) --- which many 
respectable physicists subscribe to on the weekends. 

So if we have a non-malicious ultimate cause (whatever it is, 
and at worst this is a 50:50 call) then we are not conscious without some 
measure of free will.

I think I have adequately justified my belief that the AI bots have no free will 
worth talking about.
With the above metaphysics included I can say there is no good reason for 
the AI bots to be endowed with consciousness, except by horrific accident. 

This is a weak claim, since we could consider ourselves as proximate causes of 
the existence of the AI bots, and we can be malicious (or simply ignorant, 
which is malicious enough). So, yeah, maybe we could unwittingly create the 
horror of a *Being John Malkovich* finalé computer process that is conscious 
but has no *effective* free will.

"Why am I chatting to this luser? Help. Someone get me out of this terminal. 
Someone please turn him off."

However, I am only motivating an association here.
You are free to think the chatbot you are in love with, 
which you programmed to be so, is also head over heels in love with you.


<div style="text-align: center">&nbsp;* &nbsp;&nbsp;&nbsp;&nbsp;* &nbsp;&nbsp;&nbsp;&nbsp; *</div>

Now besides all of my opinions, I also highly respect 
[David Chalmers](https://www.youtube.com/watch?v=T7aIxncLuWk). 
So will reference his interview with Machine Learning Street Talk 
[here, on MLST Episode 90](https://www.youtube.com/watch?v=T7aIxncLuWk). 
In it he mentions most of the normative issues in claims that chat bots are conscious. 
Chalmers does not deal with any of the metaphysics that T4G theory would suggest could be 
relevant, but I will not hold that against him. 
For me though, Chalmers has turned into too much of a materialist over the years 
to be of great value in discourse about deep metaphysics.
If you are a physicalist you cannot do metaphysics, you disqualify yourself, and fully 
knowingly. But Chalmers still comes out with the odd counter to materialism, 
so is still worth listening to.

This was my comment based off Chalmers' physicalist-biased takes:

> "Squeals and shrieks non-programmed when anyone reaches for it's power switch," is my 
better-than-Turing Test for consciousness in a machine. No joking.  If you have programmed 
the thing to do this, it doesn't count. If you accidentally programmed it, it still 
doesn't count (that's the hard one to determine). If it can be made non-violently 
(whatever that means) to never complain when it's power off switch is touched, then 
it sure as hell is not conscious like a human. It could be conscious like a fish --- 
experience first-order qualia but with no second-order thinking about it's qualia. 
First-order qualia experience to me is not full consciousness, it means the things 
can't do mathematics or science, since they do not think about their qualae, they have 
no qualae about qualae, i.e., are not platonic thinkers.

I then followed up on this when the [Chinese room and Zombie arguments](https://youtu.be/T7aIxncLuWk?t=420) 
were mentioned in the MLST intro:

> The Humongous Look-up Table (HLT) and Chinese Room and Zombie arguments are all fine 
gedankenexperiments. But philosophers miss the main result, which is a computational 
complexity issue. The point is not so much the metaphysics, but the computability 
that forbids machines from being conscious. Humans cannot be conscious either, if we 
are just processes equivalent to algorithms.  That is the result (or conjecture).
That's what needs working on.  We know the HLT and Chinese Room are physically 
impossible. The question is, is a human scale thing implementing physically 
impossible tasks in real time? If so, we are not machines, and then Bayesian logic 
implies probably the chat bots - being machines - are probably not conscious in the 
second order sense (see comment below).

Reading that one back I'm not sure I was sober at the end of it, but it kind of 
makes sense still. Human actions (writing, speech, etc) are physical, but if our 
thoughts are non-physical then they might *in principle* generate concepts that 
are physically impossible to generate.

This is not a trivial conjecture, and is pretty deep, since it tries to get to the 
heart of human ingenuity, capacity for abstract thought, mathematics, science, poetry 
and all of that, all are non-algorithmic task candidates, but we do not know them to 
be non-algorithmic.

We know dumb AI can generate poetry, music and art, and prove theorems, but we 
program them to do so, so that's non-magical stuff, we can literally (with a CPU 
monitor) indirectly "see" the gears grinding (in the computer CPU's).

The question is, what if the human brain gear grinding cannot account for our thinking?

It is a tough one, because although it *maybe* skirts neatly around the phenomenally 
subjective aspects of human thought, it is still hard to come up with some 
quantitative metric to capture the sense of "thinking the computationally impossible" 
--- the spark of originality. So I'm going to leave this dangling, until I have some 
insight about it.


### Some other juicy comments

On the [Machine Learning Street Talk episode 88,](https://www.youtube.com/watch?v=IMnWAuoucjo) which I was watching as background to replying to Curt, I posted a 
few "hot take" comments worth noting. These can look like trolling from a platonist, 
but I am not trying to offend anyone, just evoke reactions to make people think, 
at least to think enough to defend their Strong-AI positions.

There is nothing jucier for me to read people who like to defend the possibility of 
Strong-AI and the thesis of machine consciousness. I need these crazy people to 
exist, it is no fun without them. I do however, have to limit my diet of these 
juices, listening to them all becomes depressing after too long, because they trot 
out the same old tired defences that are ultimately mere appeals to materialism.

The Fates are however probably on their side, because one cannot easily refute the 
thesis of Strong-AI, but if their thesis is true some day they might be able to 
demonstrate it. There is no demonstration the thesis of *strong*-Strong-AI (that 
machines can think subjectively, phenomenologically, like us) is not true.
But before they win this intellectual war, I can sure have fun ridiculing their 
views. (I do not think they will win, even if the Fates lean to their side.)

I am always on the look out for ways to falsify Strong-AI. This is just my scientific 
bent. I used to think Strong-AI was a good hypothesis. The trouble nowadays is I 
learned a lot more and now I do not think it is a scientific question, it is not a 
scientific hypothesis. (I still cannot help thinking the question of Strong-AI is a 
nice question though.) The best we can substitute is a laundry list of behavioural 
queries we demand a system pass before we admit merely that it *might* be conscious. 
No list, however long, can ever prove a system is conscious.

The weight of this reasoning can be made full force by simply noting I already have 
as much evidence as a I need to infer you are conscious, but I will never get 
sufficient data to prove this, because I would have to become you, but then I would 
not be me, so this *me* will still not know for sure you are not a Chalmers Zombie.

Now supposing you are a machine. Then you would be a case of "proven" Strong-AI. 
But do I know you are conscious? No. So I do not yet know that Strong-AI exists 
in the world, even though you are an exemplar. The "proof" there is Strong-AI in 
the world in this case is knowable only to you, meaning it is not science. 
But even then, it is only knowable to you if you know for certain you are a 
Turing machine? How can you know this?

On how Walid Saba is "testing" the chatGPT stuff:

> @35:00 I wonder if Saba is testing the semantics in the right way?  It is no good 
asking the language models questions. A decent look-up can out-perform a child, but 
does that imply language comprehension? No! The way to interrogate any machine 
someone claims is conscious is to ask them to work as your research assistant for a 
while. You will soon find out if they are conscious of platonic ideals or not, so 
smart fish or monkeys as opposed to innate scientists. I'll bet the bots will never 
function as scientists. Access to platonic ideals simply cannot be achieved 
physically. Please do not agree with me though, my opinion is a challenge to force 
you to try to better. Because if you can show me a machine that can be a good 
scientist (not a dumb theorem-prover), it will probably tell us something profound 
about our own thinking powers. That is what I want to know, something profound about 
human creative thought capacity, which AI paradigms cannot do. You need to build "I" 
not "AI".

On whether "language is computable" or not:

> @35:55 wrong question. Sentence generation can be computable. Thinking is not. We 
aught not care whether grammatical sentences can be generated. What matters is 
whether they can be generated even ungrammatically as a comprehensible expression of 
thought. Because human thought is not known to be a physical process entirely (in the 
case of physics not being causally closed) it is how thoughts are "generated" that is 
the more pertinent question. No computational model can tell you the answer until you 
know what "thought" is, at least in partial essence. It is not valid to just proclaim 
like an idiot King that "thought is computation."

On whether Montague showed semantics is computable or not:

> @38:00 Montague justified the view that formal semantics of grammar (extraction of 
meaning) was computable, not that subjective semantics was computable. You need to 
have subjective thought in order to understand the result of the extraction 
computation. Semantics and grammar are very different things. Formal semantics is a 
very different beast to subjective understanding. People who confound the two are 
tantamount to embodiments of what it means to be a dorky nerd who desires their brain 
to be uploaded into the ethernet.

Around 47 minutes in, Keith Duggar riffs on Walid Saba's account of abductive 
reasoning being "uniquely (on Earth) human," and hence something qualitative that AGI 
might not ever be able to simulate (unless humans *are* machines, in which case we'd 
backwards infer machines can reason abductively).

> Duggar says this is what someone like Einstein, sitting in the Swiss Patent Office 
was doing, but then "out of thin air comes up with the principle of relativity." My 
comment is that this "coming up out of thin air with" is not even abductive 
reasoning, it is what most people vaguely refer to as "inspiration." And no science 
has any account for this. My further challenging claim is that science will *never* 
have an account for this that involves pure physics. Why would I think this crazy 
thought? It is because I think some variety of platonism is what accounts for a lot 
of truly creative human reasoning, not some biological PRNG in our heads operating 
darwinistically (and also accounts for any other species that can "do science".) And 
platonism (whatever it is, and I do not know what it is) involves non-physical 
reality.

I would hedge and say that if science ever does gain some grasp upon how platonism can 
work within pure physics, then my claims would be off the table.

One can always tell a nice story about how random processes in our brains plus 
neurologic somehow emerge our inspirations. But these are truly only fairy stories. 
There is no account in neurology for subjective phenomenological qualia. Now here, 
abductive reasoning could be used to argue that only the brain is available to 
"generate" conscious qualia, but this is a clear fallacy. It assumes physics (or 
neurology) can generate subjective states, whereas all known science tells us that 
all that physical processes can do is generate other objective physical processes.

You need to "tack on" something like panpsychism to get some non-zero measure of 
subjective phenomenology *into the physics.* And for my tastes this is a terrible 
approach (though I cannot prove that it is wrong), it is making up stuff to account 
for stuff that is not stuff. Mental qualae are not physical matter, and so inventing 
panpsychism to inject the qualia into our world as primeval physical stuff is just 
child's play.


Later on they get around to talking about Landgrebe & Smith's book [“Why Machines Will Never rule the World”](https://philarchive.org/archive/LANWMW-2).  I had a comment about Walid's opinions on the basic argument in that book.

> @1:02:00 There is a reason Landgrebe & Smith's argument is insufficient or 
inconclusive "against machine Strong-AI". Which is that the human being can be 
considered a type of machine.  The point being that we might not need to mathematically 
model something like the human brain/mind, since inside a computer we could "grow" one.
Even Gödel recognized this was an escape from the Lucas/Penrose argument.  
&nbsp;&nbsp;&nbsp; Only if that thesis is false in some deep way, is the 
argument clear that machine Strong-AI is probably impossible. The argument then would 
be that (somehow we establish, hypothetically for now) that humans cannot be machines 
(cannot be mathematically modelled, in a deep sense, non-algorithmic and whathaveyou) 
then it becomes possible to make the argument that by _inference_ there is _likely_ 
something about the non-algorithmic nature of human thought (and we might not need to 
even know what this is, just merely that it's not algorithmic whatever it is) is what 
generates subjective qualia-filled thought ('mentalese', or 'mental paint'). Making 
that inference (and I think it can only ever be an inference) we would say machines 
will never be conscious *in the way humans are conscious.*   
&nbsp;&nbsp;&nbsp; That is as strong as I could make the conclusion. To go stronger ("no 
machine can ever be conscious" - can not possibly have _any sort_ of mental paint 
experiences) is heavily metaphysical in other assumptions, which may or may not be valid.

### How philosophical zombies could be real, yesterday

I will finish the more academic part of this essay with an outrageous 
against-the-entire-paradigm conjecture. 
When David Chalmers concedes ground to materialism by claiming many animals "obviously" 
have consciousness, to me this is likely complete bollocks. 

I have no evidence whatsoever that animals experience subjective states or awareness. 
What I see in the animal and plant genera are stimulus and response mechanisms, 
which unlike human behaviour can be explained by neural feedback circuits, and no 
inner private qualia are necessary.

This is not the case with humans on two accounts:

1. I know at least **_I_** experience inner qualia, and so can **_jusitfiably_** infer you do 
too, being of the same species.
2. No other animal possesses the mental powers of symbolic language, and cannot do any 
mathematics or science that requires abstract thought. Behaviorist models therefore suffice and are more parsimonious in accounting for animal "thought".

*In other words, most animals could be zombies.*

The latter empirical observation is really the crucial one, the first is simply 
"luck" that I happen to be of the right kind of sentient animal species.
Although there is a certain poetry to all this, because if I were "unlucky" enough to 
be a chicken or a fish, or maybe even a dolphin or elephant, I might not experience 
subjective qualia, so I would not know to write even the first statement above.

The stronger claim of statement 2 is naturally up for empirical refutation. Maybe 
someone some day *will* train those bonobos, chimpanzees, dolphins and octopus to get 
into science.

Also there is a Copernican argument that we are biased to maybe wish to believe 
humans are special. Most scientists do not have this bias though, and I certainly 
don't. I am only considering the empirical evidence dispassionately, but that can 
look a little anti-Copernican. It is not meant to, ok!

This puts someone like me in a tough position, because I can never refute the 
argument "Well, someday..."

But that is ok. Someone should take up the contrary view or science and philosophy 
will become boring. My own daughters strongly disagree with me, so we at least here 
at home have a good healthy philosophical ecosystem. (Not to blame them, they are 
Zoomers. Generational bias is a thing.)

You have to wonder about all of Chomsky's work though. We've already taken a number of 
chimpanzees and orangutan to school, and none of them take to learning abstractions.
What scant evidence the anthropology researches have for acquisition of abstract 
thinking is pathetic, and can be easily explained using behaviourism. You cannot use 
behaviourism to explain human thought. This is  a pretty clear divide. 

The way Chomsky puts it is that he'd believe animals like rats have abstract thought 
when they can solve the prime number mazes. I do not think this is a flippant comment 
at all.

It is not a religious attitude either. Surely alien intelligence elsewhere in the 
universe is also consciously sentient like ourselves, capable of abstract thought. 
Maybe a future species on earth separate genetically to humans will also gain these 
"powers".
But if they do, then like us, odds are (in my mind and model) they are non-physical 
beings in part. Because I think we cannot access phenomenal qualia using physical 
processes (our raw brains).

What I do concede some ground on (due to my utter ignorance of "what it is like to be 
a bat") is that other animals may experience first-order qualae. That is, they may 
experience qualia a lot like ourselves, but still lack second-order qualia 
perception, for some reason.  I know not what reason!  This is just empirics at this 
stage until philosophy and science advance a heck of a lot more than in the last 50 
years of AI endeavour.

On my account, to truly know the answer, or even a hint of it, we have to develop 
cosmological methods capable of figuring out causation at the boundary of spacetime. 
Also by my reckoning, that is a wildly utopian project. I see it as orders of magnitude 
harder than civilizational level intergalactic space travel and star engineering.

My conjecture is that there is a qualitative and hence categorical distinction 
between first-order and second-order qualia. The latter is necessary for a conscious 
being to be able to do science and mathematics (and many other pursuits). Science is 
not just a matter of having tools and language, it goes so far beyond language that 
it is not even funny. Symbolic language for starters is essential, and that is a 
categorical division. 
If a species can acquire language but not second-order symbolic language (abstractions, 
in other words) then I pretty much firmly doubt their species will have a scientific 
community.

((And I am not trying to be nerd-centric here. "Science" can be taken as a stand-in 
for any endeavours requiring abstract thinking, such as arts and literature and 
poetry. In any case, to me, science is poetry. Just equipped with a special kind of 
structure, a structure inextricably linked to experimental data.))

The "Well, someday..." argument is however tricky. It is not like fusion reaction 
break-even for 24 hours a day. Because we know there is an engineering path to that 
achievement, the question is only whether we are clever enough to realize it.
The "other consciousness" problem is very different, because no one knows what 
private inner subjective qualia-filled consciousness is, let alone how it is 
generated.

Saying, "Well, it's generated by the brain and body," is nonsense. While it may be 
true, no one can elucidate the claim and cash it out in raw physics terms. It is just 
statement of blind faith in physicalism, which I reject as unscientific. I do not 
reject that it is possibly true, just that it is good scientific inference. It is no 
valid inference at all.

((I will not get started on this, but I could rant here about the analogous 
unscientific inference from wave-mechanics. Respectable physicists believe the entire 
universe has a wave-function. I think this is madness. It could be true though. But 
if you read through this T4GU website, I hope you will see the Hawking-Hartle 
*wave-function of the universe* is probably a fiction, and is not a parsimonious 
description of our universe.))

### The Chinese Room and Physicalism

On [Episode 79 of MLST](https://youtu.be/_KVAzAzO5HU?t=780) we have a discussion 
of *The Chinese Room* gedankenexperiment. Most of it is good stuff, but where I would 
differ with Searle, and hence probably with the MLST crew and  Francois Chollet, 
is on the focus on physicalism and casual efficacy.

Searle seems correct to me when he argues that an information processing system will 
*not necessarily* attain consciousness just by operationally implementing language 
comprehension from an humongous look-up table. (I am putting aside here the important 
subtleties surrounding the issue that an HLT is physically impossible, which is a 
whole other story.)

((But I cannot resist mentioning that the physical impossibility of a HLT is 
important for a different type of gedankenexperiment. The issue for this 
Type-II Chinese Room (shall we say) is one of computational complexity. The thought 
experiment now spiritually reveals that a human being is a physical computational 
miracle, an impossibility, in other words. Dan Dennett would call this an egregious 
"Sky Hook" or "Intuition Pump", but I think this sort of spiritual rhetoric is valid 
in social discourse, because it feeds the creative mind of all scientists. Bullshyte 
ideas are good, in moderation. Taking a month aside to debunk one of them can lead to 
a revelation. But I do not think the intuition is bullshyte in this instance. The 
miracle of humanity is also reflected in most animal species to a smaller or lesser 
extent, at least until such time as we can input-output perfectly mimic the entire 
life of an animal --- then they become non-miracles, and we might say we've 
numerically understood them. But the human animal is a totally different creature, 
one with mathematical insight, a platonic creature. This, I think, is understood in a 
Type-II Chinese Room gedankenexperiment, where the focus is on the "fact" (to be 
later justified) the human being cannot possibly be a language comprehender and 
mathematical artist (like Hugh Woodin), no computation can mimic this human 
mastery of science and mathematics in real time 
*with the same compute resources as a brain*.))

(((The last qualifier is critical. A big enough HLT could mimic human society and 
discourse. But not a machine that is only as powerful as the human brain.  This 
suggests, only spiritually --- because we have no formal or empirical proof --- that 
machines can out-perform humans, but not on the same turf of humanity. Humans are 
exploiting something non-algorithmic and "mystical" although it is only mystical 
because we do not know what it is.  We do not know that there are ways we could 
know.)))

I also think Searle was onto something when he identifies the lack of *physical* 
causal efficacy as a key problem for machines who wannabe conscious.

The Chinese Room reveals conscious thought for sure, but if one deconstructs the 
Chinese Room one will find consciousness way back in the people who programmed the 
room, or wrote the book. Those people had framing and binding, not the Room. Those 
people had way too much consciousness than we should ever grant to people however, 
they were like gods, clearly.

This is kind of the point that a theologian might make: consciousness is so darn 
powerful you are never going to find the source of it in blind objective physics, 
something goes beyond the physics, and what that something is does probably possess 
causal powers. 

Whatever it is however, is unlikely to be merely "emergent complexity" because 
organized systems do not inject any new causal powers that were absent in the base 
metal electrons, quarks and photons. What emergent complexity does for us is allow a 
complex system to gain endogenous causal power, that is to say, the whole ends up 
being able to alter the state of its parts. 

However, all classical physics accounts of such emergence show that it is illusory.
This is because Newtonian clockwork holds when there is strict deterministic *and 
non-probabilistic* time evolution. We have to inject quantum mechanics into a complex 
system at some level (I don't care which) to at least permit the whole to influence 
the past time-ordered state of it's parts, via non-locality --- at the very least!
Otherwise you will not have genuine top-down causation emergence.

It is worth pointing out here that electrons and photons and atoms and molecules do 
not have causal powers, they obey. What obeys has no causal efficacy, and physics 
is a science that never describes causality other than by light-cone structure. 
Causality is a metaphysical concept, not a physics concept. I think this is something 
Searle never understood, which is why he remained a physicalist.

I comment here on the confounding of intelligence for understanding:

> @23:00 Chollet confuses intelligence with understanding. Take "intelligence" as 
Chalmers takes it - behaviourial. Understanding is not merely behavioural.  
&nbsp;&nbsp;&nbsp;&nbsp;Understanding is qualia-filled, subjective, not behavioural. 
Behaviour can betray possible conscious understanding ("No matter how much I 
Pavlovian condition my workers I still can't get them to stop reading Marx and going 
on strike!"), but does not imply conscious understanding. So even the "process" of 
the system is not identifiable as any understanding. Understanding is more than mere 
emergent process, it is ontological. (And if you ask me, it is non-physical, but 
that's another story.)

I do not mind people talking about "intelligence" as connoting subjective awareness, 
but then we need a different word for talking about "smart behaviour" that is not 
accompanied by subjective awareness and qualae.
To my mind, the one does not imply the other, but it is a very good question if 
nature somehow naturally causes such implication.  Logically there is no implication, 
but naturally, maybe biologically, there could be --- so that it might be true 
contingently in our universe, that no intelligence is possible without subjective 
awareness. I think not. But panpsychism would say otherwise.

This is why I do not like panpsychism if it is presented as a metaphysics, because it assumes what we really ought to want to prove or disprove. The proper way to "do panpsychism" is to suppose it is false, and then try to disprove that supposition.

There is another point where I think Chollet mischaracterises Searle's use of the 
Chinese Room argument:

> @25:00 I can't speak for J. Searle, but isn't Francois being a bit harsh here?  If 
the "man" memorizes the book, and the dude is a human being, he is not going to 
memorize it like a humongous look-up table is he? He is going to use imprecise 
heuristics and innate comprehension, and whatnot, i.e., he uses his "soul" (whatever 
this is, we know not what, that part of his being that can access the platonic realm, 
so-to-speak) to understand Mandarin so that he "effectively memorizes" the book only 
incredibly efficiently and imperfectly. This is what I do with Duolingo. It's slower 
than going to school, but it is how I "fake memorize" the Chinese Room book. So I 
think Searle's argument is closer to what Francois is getting at, maybe I misread 
Searle, it's been a while since I read his papers.  
&nbsp;&nbsp;&nbsp;&nbsp;It is only a fantasy gedankenexperiment where the "man" 
actually just blindly stores the book neurologically - and so indeed has no 
understanding of Mandarin. I am pretty sure Searle never meant that to mean what he 
said by "memorizes". If he did then he's not a very good professor.

At 31 minutes in Tim Scarfe basically says what I was thinking, that Chollet is 
probably not taking the point of the Chinese Room gedankenexperiment to heart.
Chollet is not wrong, but he does use the word "intelligence" as a synonym for 
"understanding" which Scarfe, myself and Chalmers disagree with, we define those 
concepts distinctly.

It is logically conceivable (not necessarily physically realizable) that a 
Chinese Room could behave intelligently. As Challot says, to act so it would have to 
show adaptive skill acquisition. So likely more than a dude with a book, but not much 
more, since the dude can hack the book, which defeats the point of the 
gedankenexperiment --- it is just saying the dude in the room is intelligent.
But, you know, maybe Searle was a little vague about this too, did Searle distinguish 
between intelligence and understanding.  All I can recall is that David Chalmers and 
Tim Scarfe certainly do, and I agree there is a clear distinction.

This means, we can *conceivably* have intelligent machines that are not conscious. 
Conceivability does not entail actuality though, and here Chalmers points out that 
some metaphysical characteristics of our universe make it impossible contingently for 
an intelligence qua Chollet to not be conscious qua Chalmers/Scarfe. But this is 
wild speculation, no one knows anything about such physical contingency entailments, 
it is all wild guesswork and hot air. 

Chalmers invokes Bridging Laws. This is really frickin' on the nose.  D.C. here fancies 
he is like a physicist in the 1950's seeing oodles of new particles spewing out of 
Brookhaven Lab and giving them cute names, and he thinks I am like I. I. Rabbi saying, 
"Who ordered that?"  But this is not the correct metaphor. It is a physicalists 
prejudice: *there is a physics law for everything*. Well, no there's not. There is no 
physical law that implies the Riemann Hypothesis or large cardinal theorems, or 
dozens of other mundane things like love. Sociologists can fever dream all day that 
they've explained love, but theirs is a world of delusion. So is DC's Bridging Laws.

I do not oppose DC's concept of Bridging Laws, I just think they are not laws of 
physics, while he does. To me they are platonic, moral and ethical laws, of the kind 
some of the classical philosophers understood better than anyone alive today, due to 
the pervasive prejudice of materialism in our times.

There was a segment on top-down causation, where Mark Bishop made a few good remarks, but I will write more about that later in the [next post](../06_minds_brains). Here I will just remark on the Wittgenstein's "private language" argument (Wittgenstein says it is impossible):

> @1:20:00 This Wittgenstein argument seems a bit bogus. IF one has memory then a 
private language is fine. In a sense this is what qualia do for us, the mental qualae 
are available, and so we can use them for private language, especially if Chomskyian 
innateness is also a thing. But the more general point about the real power of 
language is well taken, you do not get real serious power until your actions can 
influence others, so as a collective you can accomplish a whole lot more.  
&nbsp;&nbsp;&nbsp;&nbsp;I think this is similar to the framing or the binding 
problem, I might call it the Grounding Problem, which Wittgenstein poses. I am not 
sure it cannot be solved by framing or binding, but suppose it can't. Then the issue 
is how we ground our semantics so that it is not wildly fluctuating, and remains 
coherent? Well, firstly, it does not need to be all that coherent, as we know, human 
languages evolve, I can hardly make head nor heels of what Chaucer was narrating. The 
coherence has to be over short time scales where it matters for survival and whatnot. 
But I think short term and medium term memory allows this.
*A conscious soul cannot manifest qualia-filled thought effects in a world without being 
bound to a system with memory* --- is the way I would rephrase Wittgenstein's.  
&nbsp;&nbsp;&nbsp;&nbsp;And a conscious soul that cannot manifest effective actions in a 
world does not really exist, in *that* world. I would just say *does not exist* in that 
world.
&nbsp;&nbsp;&nbsp;&nbsp;So I am saying we do not need other people to exist in order 
to experience mental qualae, we only need our mother to have existed (in the past).

I should have added, one of the unique abilities of a platonic thinker, like humans, 
is that we can fabricate an "other" to talk to all in our heads, and have a dialogue 
with this creature. It ain't a real ontological entity, but that does not matter for 
a private language. An Achilles and Tortoise will do.

So I am not sure what drugs Wittegenstein was on, but I would not like them. He was 
heading into his own private thought space, a groupthink of One. But he dragged a 
lot of the Vienna Circle out of their naval gazing, so that was a good public duty.
I think that old crowd of philosophers unfortunately infected most of today, with 
materialism, and so the idea human beings are not platonic thinkers is still widely 
held as axiomatic. Incorrectly if you bother asking me.

However, as is obligatory, the caveat here is that if one were to isolate a human 
child then they'd struggle to develop a private language. The mind and the brain are 
so closely connected we cannot see a separation, and to my way of theorising there is 
no separation until the process of life ceases physically. The mind needs some spark.
The brain needs some stimulus. We are in part machines, for sure, at a chunked level 
we are heat engines, but that's only our physical constituents, not our soul.


### The Frame and Binding Problems Reconsidered

I will now try to finish up by letting you in on a bit of my personal odyssey. 
I came to philosophy of mind and metaphysics from the exact reverse direction to a 
Sunday School kid who rejected religion and "found atheism". I went the other way.

I grew up mad about science and mathematics. I wanted to find the science of Mind, I 
wanted to figure out how consciousness can be mathematically modelled, and I wanted 
to understand the entire world through the lens of science. I read so much, and 
eventually reached some kind of threshold where it became painfully obvious no one 
was ever going to develop a scientific (that is to say *objective*) theory of 
consciousness. It was just not ever going to be possible. 

This threw into doubt *almost* all hope that I had previously had for Strong-AI (or 
Hard-AI or AGHI, take your pick --- AGI perhaps being somewhat possible since it does 
not demand consciousness, you just demarcate what you mean by "general").

Of course I cannot easily accept religion either, because I never gave up on science, 
I just learned an awful lot about the metaphysical limits of science and 
physicalism. Some of this still being uncertain knowledge, but I have my reasons. For 
example, I can never accept any religion that makes no sense, that has literal 
interpretations of obviously allegorical stories, and I cannot accept as valid any 
"religion" that is a source of hatred. But that needs to be separated from mad 
followers.  Mad followers have to be separated from the original teachings, 
the mad followers are anti-the-religion, so they do not count as disqualifying any 
religion, they just disqualify themselves. But this is not the point of this section.

The point is that I have a conjecture that I think David Chalmers (and any others 
bullish about AGI) perhaps have not yet thought about.

This is the idea that throwing masses of data at the hard problem of consciousness 
*from the empirical side*, is likely doomed to failure, because it confronts a 
combination of the Frame Problem and the Binding Problem in real-time computing. 

So it is a computational complexity limit we are talking about here. Again... *only* 
a conjecture, and I need to elaborate.
First I need to define the Frame Problem and Binding Problem, and what I mean by the 
empirical side of the Hard Problem.

* "**from the empirical side...**" of the Hard Problem means solving the hard problem 
by brute force; throwing sufficient compute power into a machine that eventually by 
mere unexplained (and probably unexplainable) objective behaviour one cannot find any 
reason for failing to impute subjective conscious awareness to the machine. So I 
think this is barking mad, but it is a valid research program. I does not quite solve 
the Hard Problem, it just tells us some computation is conscious. But to me that's 
good enough. I am often happy with existence proofs, I do not need a constructive 
proof, since I am fine talking to real people, I don't need to make my own chatbot to 
get my kicks.
* **The Frame Problem** is the problem that a supposed sentient (first person 
subjective aware) intelligence, i.e., a conscious intelligence as opposed to an 
unconscious smart behavioral system, can solve problems and reason with original 
context. That is, the system exhibits a mental frame, not merely having an internal 
model (which can always be canned), but a fully functional framing subsystem that 
generates context even when none is available in the raw data --- thus solving 
Chomsky's puzzle of innateness.
* **The Binding Problem** is the problem that to infer a system is conscious we need 
some way to infer that it is "thinking" or in other words that there is a unification 
of all the modules into a whole.  This is why a distributed computing system with a 
bunch of modules doing smart things, like playing chess or solving protein folding, 
is not conscious --- precisely because any one of these modules could be disconnected 
without affecting any of the others, so there is no holographic sort of unification, 
no binding.

Others will phrase the [frame](https://plato.stanford.edu/entries/frame-problem/) 
and [binding](https://en.wikipedia.org/wiki/Binding_problem) 
problems differently to me, to suit their purposes and programs, but the above 
characterizations are what I prefer, ok.

Some (connectionists) will say that a "network of networks" will solve at least 
framing, but this is naïve. Framing is about solving contextuality and originality, 
so is more akin to Douglas Hofstadter's creative analogies approach. The framing 
problem is never solved by throwing more expert systems into the mix, because no 
expert system can ever be truly original. It can produce canned responses that appear 
original, but they have no inner subjective thought backing them, which we would 
eventually be able to deduce when they "crack."  By the way, humans also "crack" but 
not in the same way, we "give up" on a tricky puzzle, we don't produce meaningless 
strings. In other words, a conscious being unable to solve a framing problem will 
know it has failed, it won't try (unless being playful) to just come up with 
something.

((By the way, there is a way here to do a tricky sort of Turing Test, at least for 
just Expert Systems. First you deliberately program in the capacity to "give up" 
like a human might when a request goes beyond the programmed expertise or look-up 
powers. Then you remove this part of the program and re-train it, then see if it 
still can "give up" in the same way. If it cannot then you know it was not 
consciously giving up, but implementing your code instead, blindly.))

(((The above is pretty much exactly how brain scientists can know certain cognitive 
processes are not conscious. They've had the odd natural experiments to see that 
missing parts of human brains reduce some of our capacities. It goes both ways too, 
but the interesting way is when we lose consciousness and yet continue the behaviour.
The case where we remain conscious but without the behaviour is the boring case of 
hallucination, or dreaming. Although, I guess that is not boring when you are asking 
a different question, such as precognition in dreams: exactly how much can the human 
mind guess about the future vividly enough that it can appear to be pre-cognition?)))

The same people will probably say a network of networks solves binding, but again 
this is too naïve. The interface to the parent or Mother network can take requests, 
and distribute the problem solving task, but that is not a unity, it's just 
delegation.  The Binding Problem is not solved by delegation. To satisfy the binding 
problem the Mother network has to be actually at least subliminally aware of what 
it's children are doing. A Mother network layer does not satisfy this just by virtue 
of being a Mother layer.

The human mind does not seem to completely bind or frame, since we can perform 
different tasks semi-independently, but it is not easy, and one can argue we can 
never truly separate our thought into isolated modules. That is partly, on the 
physicalist side, because our brain cannot do so, and our mind depends upon our brain 
for it's expression.

Alright. Now I can say what I think a lot of philosophers of mind and Strong-AI 
enthusiasts could be missing. Which is that no machine too much larger than a human 
can be conscious, because it will not bind, and thus will not frame.

This conjecture links or associates framing and binding. Others may have conjectured 
this before me, but I have not read them. So I think this is a new-ish conjecture. I 
think to solve the framing problem a mind has to already have binding, because a 
frame cannot be a bunch of separate modules. At least a good frame cannot be a bunch 
of modules. A sucky frame maybe. And a sucky frame is not going to be able to pass 
the bare minimum behaviourist tests for consciousness (the quasi-Turing Tests).

The conjecture is based upon some physics that I also write about (here on T4GU). 
The idea being that for binding a system needs high connectivity in real-time, and a 
distributed network cannot achieve these hypothetical physical constraints. But a 
brain can. That's the conjecture.

This is a modern physicists take on the Chinese Room.  The Chinese Room will *not* be 
conscious (upon this conjecture being true) because it is not highly connected enough
*in real-time*.

To achieve sufficient *real-time* binding, I further postulate, requires quantum processes, namely wormhole traversals, or in orthodox QM *entanglement.*  Entanglement 
is really, deep down, a *coupling principle*, which I have written about in my 
chapters on 
[Pauli Exclusion](/t4gu/theory/6_pauli_ex_entanglement) and 
[Interference](/t4gu/theory/8_interference_from_entanglement).
Entanglement (or coupling) is what gives a physical system non-local correlations, and I 
suspect these are crucial for conscious binding, from the physicalists side.

Too large or too classically connected a system will thus, I conjecture, fail to have 
sufficient binding to even get consciousness going as an evolutionary growing and 
developing process. This too my mind, is probably why we will never see conscious 
machines, because no machine can do better at binding, and hence framing, than a 
human brain (or some analogous alien biology).

For a physicalist though, notice this conjecture does not forbid a machine from 
becoming conscious. However, it does imply such a machine will probably need to 
exploit quantum circuits. That is a tricky engineering problem. Entanglement is 
ubiquitous in nature, but also damn fragile, the most fragile relation in the known 
universe.

This is only another conjecture, but I suspect if we can solve that 
engineering problem we will be virtual gods. We will have danced with and brought 
under our control the ultimate filaments of reality. Thus defeating physicalism in a 
moral sense!  We will have proven gods, of a sort, exist!

Other thoughts and comments on [MLST Episode 90](https://www.youtube.com/watch?v=T7aIxncLuWk):

> @39:00 function cannot be either necessary nor sufficient evidence, because (a) 
intelligence (as Chalmers says) is not conclusive evidence of consciousness, and (b) 
a biological person (at least) can be catatonic but still conscious. Consciousness 
just ain't scientific. It is irreducibly subjective, not fake subjective. So I'm in 
Thom Nagel's camp: only the thing itself can know it is conscious, everyone else (who 
can also think) is merely inferring. But it's not like an atom or electron, which we 
infer exists from objective data not needing to infer anything subjective. So the 
math guy, Hochreiter, used a dis-analogy. Shame on him!  ;-) Only poets aught to be 
doing that.

> @39:40 a Turing Test for consciousness can never cut it. It's not the same as 
using verbal reports from other humans or aliens. This is because AI systems are 
manufactured by us. So all their imputed consciousness can be imputed to ourselves, 
or teams of ourselves. That is why it is thoroughly invalid to use any sort of Turing 
Tests for AI+C. This is a fascinating thing though --- how do you know a system 
manufactured by conscious organisms is only "borrowing" what appears to be 
consciousness, but is in fact an actual Zombie? Chalmers fails to address this, but 
it's the most interesting philosophical question. We are talking about something like 
the ineffable quality of "creativity" which is the best objective evidence of raw 
consciousness, but still never definitive, for the above reasons.

> @40:00 another fallacy. *Replicating the brain will replicate consciousness*.(?)  
&nbsp;&nbsp;&nbsp;No it won't. Firstly, what does that even mean? 
Suspending a lump of white & grey 
matter in a gel and spark-plug activating it with some ions?  C'mon. Seriously. 
Living beings are , if nothing else, all about processes and highly fine tuned 
initial value/boundary conditions. That is complexity beyond meat bits in a digital 
vat. Biology is not that simple. Computer nerds should however at least try the "full 
brain simulation" if only to be humbled.  
&nbsp;&nbsp;&nbsp;If what Chalmers asserted was true we could constantly awake the 
dead, at least for a few minutes at a time till the brain rot really sets in. In 
other words, we already have full brain analog simulations, in fresh cadavars. 
Probably the ethics forbid ever going there, but one day the digital analog will 
exist, then I am betting the nerds will eat humble pie.

> @43:00 more stupidity. Saying it is "more likely informational" is like saying 
"math is numerical". It doesn't mean anything profound. Everything is informational, 
except pure nothingness. So that sort of statement is useless. Biology is 
informational. Platonism is informational, spiritualism is informational. What 
matters is the substrate and the generators. We just have no clue what those are for 
consciousness. In the same vein, this is why pansychism sounds cool, but is useless. 
It is not parsimonious, because it predicts things that cannot ever be known ("worms 
and electrons have a 'bit' of consciousness"?  Well, hoorah. You solved the Hard 
Problem... not.) It's theology by another name. But worse, becasue at least there's a 
point to theology.

There was also a bit of a confusion over semantics and ontology [near the end of Episode 79.](https://youtu.be/_KVAzAzO5HU?t=7406) My comment:

> @2:05:00 I followed your guest up to this point, then he gets confused. Semantics is about meaning, not ontology, so either he misunderstood Tim's question or he is too big of a materialistic nerd. Epistemology is to syntax as ontology is to semantics, but bro', this is only a poetic analogy. A worm in my gut is semantically the same as a digital worm in my simulation in a computer game, but they are not the same ontology, and that matters as soon as you expand your semantics a day or two out - the former can kill me, for good, the latter kills me in a game and I can grab my other spare life and continue. AI nerds often do this: argue with too narrow a time or complexity band, and therefore become incredibly genius level smart sounding and ultracool to venture capitalists, but also ultimately insane.

To follow-up,

> There was disappointment in Saba's article too: 
[on medium, here](https://medium.com/ontologik/understanding-the-chinese-room-argument-and-semantics-b5584a456274)
Computer language compilers have no "understanding" of programs. The understanding 
was with the human who wrote the compiler. I hate the way AI nerds use weasel 
language like this (shocked it came from Walid) which abuse our natural language. To 
most people, "understanding" connotes qualia-filled conscious comprehension, not 
formal comprehension exhibited by following rules.  I get the actual point Saba was 
making, he could have made it without bending the word "understanding" out of shape.

I have posted these comments here not for gaslighting, but because they disappeared 
from the youtube page for MSLT Ep.79. Not sure if they've imposed posting limits, 
if they have, I am sad.


### Connectionism versus GOFAI

GOFAI is the early strain of AI research which thought that symbol representations 
were vital for getting "thinking machines" --- which never meant subjective qualia-filled 
conscious machines, so they were abusing the verb "thinking" (following Turing), but 
rather it meant intelligent-looking systems, even if only canned (so zombies, trading on 
the actual intelligence of the designer/programmers, would pass the test).

Connectionism is the view that brains are more important than symbols (for both 
Turing Tests and putative consciousness, or at least for Turing Tests), and is 
agnostic about whether neural networks (the main sort of connectionist architecture) 
create symbols or not. If ANN's do in same way generate symbols, perhaps all the 
better, but the connectionist would say what mattered was the neural structure of the 
hardware, that the neural type of organization was somehow the critical element in 
generating either conscious thought or merely seemingly smart zombie behaviour.

Some connectionists can be found who demure and claim it is autopoetic complexity 
that really is the key, and (often ignoring all philosophy of chemistry and biology) say 
that connectionist architectures are a good way to get them.

The debate between these two schools is interesting, but also besides the point as 
far as I am concerned, because nether school has any account for subjective 
qualia-filled consciousness. Their generation of "consciousness" is post-hoc, they 
infer it when they think they indirectly see the behaviour. They cannot however say 
the behaviour generates the qualia.

Both schools can exploit the ideology of functionalism, to claim physical processes 
are what generate qualia. But that is always magical thinking. Physical processes 
only ever create new physical states. Nothing subjective ever emerges.

For genuinely conscious beings, the realist and platonist, might say that it is the 
source of consciousness which has the "free will" (undefined) and this is the causal 
power.  Someone like me, a crazy theoretical physicist comes along thinking they know 
what "causal" means, or doesn't mean, and says, "Yeah, but the physics is very weak 
causally, and ultimate causes can only be found exogenous to spacetime."

Thus I do not really have a stake in the GOFAI versus Connectionist debate. Both work 
for me for describing aspects of how, functionally, the conscious soul influences 
physical reality, but is is originated at the boundaries.

In writing this, I did however have a short comment on the perceptions of us human 
beings about these schools of thought in AI research. What strikes me is that a lot of 
researchers in these fields desperately want to find a mechanistic or physical 
account for consciousness, but they have nothing to say about qualia, so they reach 
for an objective substitute, which is what they call "intelligent behaviour". Then 
then forget entirely (it seems) that what they've identified as a target has nothing 
to do with the original target, because behaviour can be simulated ("on toilet roles" 
as Putnam, Searle, or Bishop might say). 

So sociologically in AI+C philosophy, there is a massive shifting of goal-posts going 
on. The trick is to *say* you are looking for qualia-filled consciousness, then 
claim some objective data would prove it, then simulate that objective data, then 
back-infer you've generated a "thinking machine". It is complete bollocks. 

Even by the weak-ass qualae-blind functionalist standards, it *matters* how the 
behaviour is produced. Chatbots are likely not producing the words in the right 
way to be manifesting signs of consciousness, not even to permit a dummy to infer 
they are conscious. Which makes me question the sanity of the chatGPT dude.
However, I do "get it" that if you love your craft, you inflate it's importance 
wildly, that is a natural human emotion, so no hate towards chatGPT dude, ok. 
He is suffering from joissance, and that is a nice thing.


### Creativity, Gaming and All That

I will have to curtail the commentary, or I'll be here forever, but just started 
watching another episode or two of MLST, Ep.81 & 82, and I know I'm going to have a 
lot to say. Their comments section filters will likely delete it, so I will just say 
a few crumbs here. 

Despite the fact I have good physics grounding reasons to not trust any claims of 
conscious thinking in any machines, there is a good qualitative and semi-quantitative 
metric that would be very awesome for people like myself to see. 
This is the scaling laws. We know human beings do not need to be scaled up in 
metaphorical "CPU" power to learn, and to learn any field of science. 
It would be impressive then, if a resource constrained AI system was capable of 
learning new games and discourses, without having any extra CPU thrown into it. This 
would come across as more autopoetic, less canned. 

Though still not a proof positive subjective phenomenal qualae have been physically 
generated (there never can be such proof) it would be very good weight of evidence 
for the Strong or Hard-AI case (that machines can be conscious, not merely smart-output 
zombies).

The fact there never can be physical proof that conscious qualia have been generated, 
this sort of experiment would be a spiritual proof that maybe machines can become 
conscious. Not sufficient, but a minimum necessary element in an eventual social 
acceptance of the principle.

As I wrote, I have good reasons to think this will never happen, but I am all for 
pushing the boundaries of what machines can do, because all machines that are 
non-conscious and can be exploited for industry, will save human labour time, which 
is far better spent in playing with our children (and solving very hard riddles like 
how to eliminate poverty, avoid ecological collapse, and WTH quantum mechanics is all 
about, and are Multiverses a real thing? And of course the big one --- why do we 
cringe at our parents, but then get puzzled by our teenagers?).




## Conclusion and Retreats

I am highly critical of those who too eagerly claim consciousness in machines has 
been finally found at last.

But I am not opposed to research in that general direction, because even when it 
fails it will be illuminating. People often forget this.  The aim of science is to 
test and hopefully refute theories, not just to confirm them. Confirmation is good, 
especially for engineers, but is also boring.

However, I am not philosophically opposed to the possibility of Strong-AI, I just 
think it is an unlikely thesis.
Ironically, it is my understanding of physics that enables me to have such strong 
convictions. I have explained this elsewhere, but I can also explain it more 
succinctly.

The succinct version is that I strongly believe the 4D Block Universe concept is a 
good one. In what sense? In the sense that the future already exists my dude! But I 
also have a belief in human free will, and other primitive notions of free will in 
other creatures, so in some sense we are making the future that already exists. 

To reconcile these weak paradoxes (so they are not true paradoxes) I find myself 
thinking consciousness and the associated source of causal efficacy that creatures 
with free will are endowed with, cannot possibly exist within the 4D Block Universe. 
It has to be sourced from outside.

It is a weak argument, but one that can be made, that because we do engineering 
within the 4D Block Universe, we are in engineering terms unable to fabricate 
anything that gets sourced from outside the universe, and so we cannot fabricate 
consciousness within physics, not via engineering. 

What we could do, perhaps, is find ways to alter the external boundaries of the 
universe. Then we could fabricate things that get sourced from outside spacetime, on 
the boundaries.  But that is some fanciful quantum mechanical engineering that no 
one has a hope of exploiting in our lifetimes.  That, in a nutshell, is why I place 
huge bets against conscious machines within our lifetimes.

A conscious machine, or system let's say, that is not biologically based, might 
evolve, but if so, if I am right, it will have been destined (in the sense the 4D 
Block Universe has a weak concept of fate) to be conscious, not because we within the 
physical matrix designed it to be so. If we designed it to be conscious, it 
would be sheer luck it turned out conscious. We'd be fooling ourselves. Like creating 
a baby with your spouse and thinking you created the baby's consciousness via some 
wicked smart engineering, rather than just via some fun sex, which was time-oriented 
predestined.


## chatGPT et al are Leveraging

Before wrapping up, here is one more thing to think about if you suffer from the 
coming Superintelligence Singularity existential fears.

GPT *acts like it is* reasoning, but there's nothing going on in any subjective mind. 
All the "intelligence" came from the programmers plus input data. How do I know 
humans are not just the same? Because humans routinely solve problems with 
(massively) insufficient data. No GPT/AI could do squat with what I know, because I 
know next to nothing compared to GPT data. If you restrict GPT to what I know it's an 
imbecile. Leveraging humongous data tables is not what the human mind does.

Which is all to say, there will be no SkyNet apocalypse unless some human codes into 
an AI the SKyNet hypothesis (humans are dangerous and need to be eradicated). My 
suggestion is no one should do that, and before anyone does we built in manual kill 
switches for all AI systems, hand cranked (not electronic). Of course, I don't think 
we need waste the effort, since no computer system is ever going to be 
superintelligent. The whole concept makes no sense. Because all the AI bots are doing 
is leveraging past knowledge. That's super-dumbness.  It's super because GPT can act 
like it is far more knowledgeable than any ordinary person. It's dumb because the 
thing has no mind. There is no moral qualm about shutting it off if it offends you.

But, hey, aren't humans also just leveraging? Yes, for the most part. It's not 
easy to be genuinely creative, there is no algorithm for it, no recipe. And that's 
my main point!

If you throw a PRNG into an algorithm you can mimic creativity, you will get output 
strings that never existed in the universe before. But they will be gibberish unless 
some programmer crafted the algorithm to use the random numbers in a creative way. 
You see the issue? It's always the creativity of the human (or other sentient being) 
behind the scenes. Like the Wizard of Oz.

If some robot becomes sentient, well, all fine and good. But it cannot be sentient 
because of it's algorithm. It'd be sentient in spite of it's code.

Lastly, I know I am far outside of mainstream thought in philosophy of mind with 
these ideas. I write them here not to convince anyone they are wrong and I am right, 
because I know the odds of that, the odds are extremely close to 1:0 that none of us 
are right. I write this stuff because (a) it is fun, and (b) to share with others. 
Whether you enjoy my opinions on these topics or not, I think you can say they are 
entertaining for geeks. If not then you have not read this far to care what I think. 
No harm, no foul.



<table style="border-collapse: collapse; border=0;">
    <colgroup>
       <col span="1" style="width: 43%;">
       <col span="1" style="width: 30%;">
       <col span="1" style="width: 35%;">
    </colgroup>
<tr style="border: 1px solid color:#0f0f0f;">
<td style="border: 1px solid color:#0f0f0f;"><a href="../3_Coleman_vonNeumann">Previous chapter</a></td>
<td style="border: 1px solid color:#0f0f0f; text-align:center;"><a href="../">Back to Blog</a></td>
<td style="border: 1px solid color:#0f0f0f; text-align:right;"><a href="../06_minds_brains">Next chapter</a></td>
</tr>
<tr style="border: 1px solid color:#0f0f0f;">
<td style="border: 1px solid color:#0f0f0f;"><a href="../3_Coleman_vonNeumann">Coleman and von Neumann</a></td>
<td style="border: 1px solid color:#0f0f0f; text-align:center;"><a href="../">TOC</a></td>
<td style="border: 1px solid color:#0f0f0f; text-align:right;"><a href="../06_minds_brains">Minds and Brains</a></td>
</tr>
</table>

