---
title: "Sabina Caveina"
date: 2023-03-10
toc_depth: 3
draft: false
katex: true
toc: true
disableTitleSeparator : true
---

{{< Sabina_2 >}}
> "Sometimes the only scientific answer we can give is ‘We don't know’."  
--- Sabine Hossenfelder, from 
[Existential Physics: A Scientist\'s Guide to Life\'s Biggest Questions.](https://www.goodreads.com/work/quotes/93999759)  


I love Sabina Hossenfelder, she's one of the emerging great science communicators and 
popularizers of our generation (go Gen-X, we're not done with extinguishing our 
neoliberal heritage just yet).

However, this week she had a major fail. Fortunately it is in philosophy and ethics, 
so not a big deal.${}^\ast$

I'll write about this here (even though it has nothing to do with T4GU) just in case 
the YouTube moderators delete my comments on her 
[video here](https://www.youtube.com/watch?v=cP5zGh2fui0).

${}^\ast$ (That's a joke. Read why later.)


## The "Understanding" Question

For cringe posterity, I will quote my comments on Sabina's video here. No filter 
(except spell-checking).

**First comment:** Before correcting the input-output thinking bias @6:49 you were 
grossly using a behaviourist black-box definition 
of "understands". So sure, they "understand." But not by common definitions, which 
tend to imply subjective mental qualia (or quale). ChatBots do not experience such 
qualia, at least I suspect not, I did ask them, they said "no" and expanded at 
length. 

It's easy to warp a definition of 'X' and claim a system 'has X'. If you over-simplify 
what you mean by "understands", making it an operational descriptor, then it proves 
nothing profound about these *inference from past data* systems. They're not even 
"intelligent," like most people they just have basic inference compute capacity 
albeit which is of a different type to human inference capacity. Humans use a 
ton of heuristics, and of course draw upon qualia quite a bit, which is totally 
different data to a numpy array.

**Second:** BTW, chatGPT totally failed on some simple questions related to my own 
theoretical physics research, it pulled stock undergrad level stuff out and dressed 
it up, so it was not even "comprehending" stochastically. It could have just output: 
"Oh, I have no clue about your research, and have not studied spacetime algebra 
closely, let me read it a bit more and get back to you."

It didn't, largely (statistically) because that's not how it has been trained to 
respond.

For any other narrower and more common field of knowledge I am sure chatGPT and 
openAssistant will eventually mimic a human pretty well. Operative word being 
"mimic".

**Third comment (longer):** Try typing this into chatGPT:  
"Does chatGPT as a complex distributed system, the software system I am writing this 
in, experience subjective mental qualia (or quale), as David Chalmers or Ned Block 
would define ‘qualia’?"

One answer circa.2023-03:

--- 

<span style="color: #77ccaa;">
"As an artificial intelligence language model, I do not have subjective mental 
experiences or qualia. Qualia are subjective conscious experiences that are unique to 
each individual and are typically associated with sensory perception, such as the 
experience of seeing the color red or feeling pain.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chatbots, including the software system I am 
running on, are programmed to simulate human-like conversations by using natural 
language processing algorithms and rules-based systems. However, they do not possess 
consciousness or subjective experience. They are designed to respond to specific 
inputs and generate outputs based on predefined rules, algorithms, and data.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;While some researchers argue that advanced AI 
systems might one day become capable of subjective experiences and consciousness, the 
current technology is not advanced enough to create machines that experience qualia 
or subjective mental experiences."
</span>

---

which is a very good distillation of current human civilisation scientific and 
otherwise knowledge. You trained on the data well grasshopper. 

The Turing Test was only designed to measure something vague we label "intelligence" 
(which of course I accept, I do not mind vague definitions for such things) but it is 
never a test for subjective conscious awareness.
    
The thing is, subjective qualia is about the only thing you expect an honest answer 
about from a conscious subject. If it says it ain't conscious it probably is not. Is 
it always honest though? That question is not applicable to an amoral system.

Get back to me Sabine when you understand what "honesty" is? I for one have no idea. 
It's not merely "avoiding logical contradictions given your beliefs" because 
**(a)** a system does not have beliefs if it is not conscious (it has rules), and 
**(b)** no one has completely credible beliefs if they are being honest. 
You see the problem?


### Subjective Definitions for Amoral Ends?

I have to explain the first joke. I think when scientists (so not "science") behave 
amorally or ethically neutral (so not even immoral and not even unethical) on 
anything socially consequential it's always likely a huge fail. As Einstein (one of 
Hossenfelder's heroes) said, "Science without religion is lame, religion without 
science is blind."

That's a subjective opinion. But I endorse!  What do you say?

Let's be honest too, Einstein was using a broad definition of "religion", which 
redefines the word, differently to common parlance. Einstein meant ordinary morals 
and ethics, not the theological stuff. However a theologian can make the same 
statement. What Einstein said is false of "religion" only if we include the 
banalities of organized practice of the anti-religions. What the priests and clergy 
give us is largely anti-religion, not religion. Religion (properly defined) is a 
source of good, not evil. But this is no longer the common definition. The common 
definition is confused and confounds all the anti-religion nonsense under the same 
banner. Which is pretty gross. All the evil done in the name of a (possibly valid) 
religion is anti-religion. The organized priest and clergy are thus often 
anti-religion.

Science has massive impact on society is my other point, arguably far more than the 
arts (maybe? I'd hate to judge, and it depends on what criteria one uses). If you 
take a morally neutral stance with your science, you potentially do society grievous 
injustices, and only if you are lucky will you be avoiding horrific consequences.

This is not to say infusing your science, especially the communication, with moral 
and ethical uprightness is free from horrific consequences. People can have extremely 
warped sense of morality. The point is what is the best you can do?  The best is 
to uphold morals and ethics and that means being honest with your science 
communication (among other things).

In the last analysis then, Sabina did not totally fail, because she is clear that her 
opinions on the status of "understanding" being ascribed to the chatBots was only her 
subjective opinion, and she admitted this.

It is a gross error in reasoning to claim mounting evidence of ‘X’ is mounting proof 
of ‘X’. Anyone studying the Riemann Hypothesis or Goldbach conjecture appreciates 
this. (Or read Nassim Taleb's 
[The Black Swan](https://www.fooledbyrandomness.com/).) Personally, I see growing 
coolness and awesomeness of the chatGPT, copilot, 
openAssistant, knowledge engines to be brilliant engineering accomplishments, but 
completely unconvincing evidence in favour of 
the hypothesis they are conscious or "understand" anything.

I am, of course, using a different definition of "understand" to the one Sabina used. 
By her definitions her (stupid) opinion is perfectly justified.
I say "stupid" because it is dropping the ethical ball when a respected science 
communicator uses common language words but with uncommon geeky meanings and applies 
them inappropriately. Claiming sophisticated chatBots "understand" by your simplistic 
definition is unethical, or at least ethically neutral, and fuels the fever dreams of 
the Singularity nerds who (might?) think wetware humans are trash and should be 
eradicated from the face of the good green Earth.

Yes, I am aware Sabina's audience are mostly science geeks. But even then, I think it 
is still unethical to bandy about the word "understand" in a materialistic geek 
sense, because it feeds the groupthink of the science nerds. They need exposure to 
wider culture from time to time, for moral education reasons. If you think groupthink 
in so-called "science" communities is not a thing, just look into the Singularity 
Institute stuff, or the Finitism mathematics community, or Neoclassical economics.


### Qualifiers

Notice in my first comment I get to 6 minutes into Sabina's video, where she 
correctly points out the "input--output" behavioural definition is dopey. It does not 
capture what we all mean by "understand." So not a fail for Sabina here (as if I'm 
any great judge, ha!). Maybe I was too hasty in judging this clip?

However, a minute later she tries to *scientificize* the concept of "understand", with 
this hooey:

>  “understanding something” is the ability to create a useful
model of the thing we’re trying to understand.

No it isn't. 

The ability to model is a semi-mathematical skill, not subjective awareness or 
understanding. Hossenfelder is still using a behaviour definition, it's still gross.

**Hence a fourth comment:** @7:20 fail here. The common meaning would imply: *_the 
ability to create a useful model of the thing we’re trying to understand_* is a 
partial potential necessary condition for "understanding", not sufficient, and not 
even always realizable. I can never truly understand anything, but I can get a grip 
on some things, and demonstrate this grip with a useful model. That is not 
understanding, it is a manifestation that comes from understanding.


### The Latitudes and Lassitudes

12 minutes in Sabina provides a funny example of where chatGPT fails, in trying to 
answer whether Windsor UK is further north than Toronto Canada. It gives correct 
numerical latitudes, but the wrong answer. Sabina puts this down to the lack of a 3D 
model of the Earth "in it's chatGPT non-existent head" (my phrase), she says,

> "But it doesn’t have such a model. It only knows relations between words."

Wrong again (I think). ChatGPT does not "know" anything. It's not sentient. It encodes 
knowledge, it does not have knowledge in a subjective aware sense. These are my 
claims, and the infinite times chatGPT fails hilariously are all spiritual proofs of 
the point (not ever logical proof, because I simply cannot ever know if chatGPT or a 
Bat or a Cat is conscious. Yeah, I'm a Nagelian in this respect. So shoot me. )

It does get a bit weary having to constantly point out such definitions of words like 
"understand" and "know" when the common usages are fine and should be granted by 
science nerds. If they really mean something more like, "behaves as if" I think they 
should spill the extra ink to rite it out as such, or coin a new word. In the case of 
chatGPT or OpenAssistant a fair word to describe those systems is "information 
retrieval" and report generation systems.

The engineering beauty of these systems is how awesome their reports are, 
sometimes almost *as if* they were written by a competent human export.

Massive puzzle of inference here.... if one is trying to use such reports as 
*evidence* of sentience (subjective awareness). ((Yes, *sentience* can have, in 
some use cases, a strict behavioural or operational meaning too, so one should make 
that qualification.)) 

But the failures are negativa, so good 
counterexamples. The successes of chatGPT do *nothing*, I repeat **_nothing_** to add 
to proof the system is sentient. It's a complete failure of Popper's lessons to think 
otherwise. Confirmation for a theory is not what a scientist seeks. We seek *only* 
dis-confirmation of all rival theories. This is what the chatGPT failures do, they do 
not confirm my theory chapGPT lacks consciousness, they falsify rival theories that 
the system is conscious.

((I am not writing today about my theories for why AI systems that run computations 
are not conscious. Although I do not agree entirely with Sir Roger Penrose on such 
matters, if you look at his work you can get a flavour for how I feel about "machine 
intelligence" --- it is not  thing, not if by "intelligence" you mean an entity 
experiencing subjective mental qualia, rather than the operational Turing Test 
definition (which is valid within the context of the Turing Test, i.e., mimicry 
capacity).))

With subjective stuff you really don't want to be using inference unless as a last 
ditch resort (they're unconscious and need help). Just ask the entity, don't try to 
read their mind.

Mind reading devices (cranium electrographs, or future sophisticated 
electroencephalograms, etc.) might one day work quite well, but when they fail 
they fail hard.

Curare disabled person + Idiot:

> **Idiot:** Ah, I see on the read-out you want to be dead, righto' sir.<br>  
<font style="color: #ff6666">\[injects cyanide.\]</font><br>  
**Disabled person:** <font style="color: skyblue">\{Arrgghhh,... no, I meant that 
metaphori... \}</font> <br>  
**Idiot:** Oops.



## Good Actions

I recommend sticking to your moral and ethical principles, fwiw. In the classic moral 
dilemma of how to treat a sick patient (say terminal cancer), do you tell them, "All 
hope is gone, sorry"?" Or can you ethically, as a physician, say, "Well, your 
condition has low chance of recovery, but there is always a small chance of 
miracles."

As an ethical physician you'd always say there is a chance of miracles. Albeit with a 
frown and inward depression about your patient's likely fate. As a physicist you'd 
have no problem at all with this advice, because... Boltzmann Brains.
The physics--physician might then have a further dilemma about whether to divulge the 
probabilities.${}^\dagger$ 

On the other hand, people have profound incapacity to comprehend the meaning of small 
probabilities, so I'd maybe not even have qualms about telling the patient their 
chances of recovery, with a wide *relative* uncertainty margin bounding an incredibly 
small probability.  Let them pray for miracles. It might not do the patient any good, 
but it is bound to help their family and friends go on living in some peace with the 
tragedy (just don't ask the militant atheists in your family to pray with you, that'll 
have the opposite to a calming effect).

${}^\dagger$On such detailed matters perhaps silence is ethical.



## A Not Final Word

I [wrote about chatGPT](../4_chatGPT) earlier, but I can add a bit more, as an 
up-to-date perspective.

The, "Are the chatBots conscious yet?" question is by far the more interesting 
question, not whether they "are intelligent". Clearly they are intelligent (by 
behaviour black-box definitions), but so was Wolfram Alpha, and DuckDuckGo. But all 
the knowledge and intelligence was derived in those systems, it was literally 
leveraging human non-blackbox intelligence, nothing more. It is only if you employ 
a definition of ‘intelligence‘ that connotes subjective awareness of the ‘thought’ 
processes that we get the more interesting question. Algorithms and computations 
are not thought processes, no matter how complex, the former are of a completely 
different category of phenomena to the latter (by usual definitions).

It is the most interesting question for a philosopher, but not for a scientist.
The scientist wearing their science hat can never answer the question, so it is 
uninteresting to them *as scientists* (not perhaps *as ordinary people*).

((Are human beings also just black-boxes? If you had to ask stop reading my blog you 
idiot. Yes, to psychologists humans are black-boxes, but clearly not to biologists 
and medical practitioners, who routinely get inside us.))

What the science geek is sorely tempted to do when faced with such explanans with no 
explananda is to redefine the phenomenon. So they will (some of them) redefine 
"understand" or "intelligent thought" to be purely computational. When they do they 
make it something a black-box is capable of exhibiting, and thereby trivialise the 
problem into a physical-material question that can be answered. This is what the 
$\Psi$ people do, they trivialize "'consciousness" by associating a number to 
"consciousness." They have then completely escaped the realm of what everyone else 
thinks is the meaning of "conscious." So it is stupidity. Fair-to-useless science, 
but not answering the non-stupid questions.

Is there something "creative" about the chatBots though? This is not a scientific 
question. But subjectively, yeah, they are capable of rudimentary creativity, because 
they have some PRNG generative output. Any PRNG is going to give you (by high 
probability) something no one has ever output from their mouth or pens before. 
But that's not the same meaning as "human creativity" which is largely undefinable, 
as it appeals to subjective mental qualia, such as in dreams or "flashes of insight" 
and whatnot, which are not scientific notions. They never will be scientific notions.

((Yes, I know REM and theta brain waves are indicative of dreams, and can be 
experimentally studied, but that is not what I was referring to. I was referring to 
the mental qualia of our dreams. The brain waves are not the qualia.))

That's just by definition of what counts as "science". It does not mean such 
phenomena are not real. Science always seeks to explain the "real" but cannot, by 
methodological limitations, ever fully explain *the real*. That doesn't mean we give 
up trying. Why not? Because we do not know what the outer limits of all future 
science are.

Come up with an objective definition of "understand" or "intelligence" and then you 
can scientifically study that definition. But it is never going to incorporate 
subjective phenomenal qualia. Just admit it!

### The Subjective is Never the Objective

That's a double entendre.

In science the subjective is never the aim, all good science objectifies, and when it 
cannot is silent, must be silent. If you redefine the meaning of "science" to include 
study of the subjective you are just abusing language.

This does not mean science cannot properly study the outward behaviour of subjective 
systems. Psychology is a perfectly good, albeit immature, science.

What I've recently come across is postmodernist "cognitive scientists" who are now 
claiming, "The subjective can become the objective." Or the other way around. They 
mean this ontologically, not methodologically. They truly seem to believe 
consciousness is material and can be objectively comprehended. 
 
That is a giant category error: they are thinking in behavioural terms, like Sabina 
lapses into. It's gross.

Here is a perfectly good partial definition of Subjective:
> The subjective phenomena are (among other things) not objective.

Got a problem with it?

It means if you can identify objective phenomenology, it is not a subjective 
phenomenon.

This does not mean the subjective phenomenon is absent, it just means you have not 
grasped it's subjective component, and have identified an objective (probably 
physical or the like) aspect (brain excitations).

It is a legitimate question to ask whether any subjective phenomenon exist. Know I 
know the answer to this, but I am not sure you do. To know the answer requires 
consciousness. I know I am subjective in aspect. But I cannot say you are. Yet I am 
not a solipsist by inclination, so I merely infer you are subjectively conscious too, 
do you have a problem with that? Among other things, it gives me some extra licence 
to treat you using the Silver Rule (do not do to others what you would not want them 
to do to you).

I'd apply the Silver Rule to other creatures too, even without much inferential 
clues they might be conscious. But my believing you are conscious is, let's say, 
extra super motive. (Hey, we're all flawed and emotional.)

### The Subjective Subject is the Issue

I think I will defend to my death the rights of the working class, and work until I 
die for working class emancipation from the tyranny of labour exploitation --- this 
is the deep motive all throughout my other website 
<a href="https://smithwillsuffice.github.io/ohanga-pai/" style="color: #1585d5;">Ōhanga Pai</a>.

The scientism project of seeking to objectify human subjective intelligence, 
understanding and consciousness, is a disgusting neoliberalization of cognitive 
science in my humble opinion. So counts as part of 
<a href="https://smithwillsuffice.github.io/ohanga-pai/" style="color: #1585d5;">Ōhanga Pai</a>. 

What I mean by "neoliberalization of the soul" is many things, but one of them is 
the university--banking--industrial complex that seeks to eradicate the morality 
and goodness inherent in the *idea* of the transcendent non-physical human soul. 
You do not have to believe in the existence of the putative human soul to appreciate 
this social justice cause. You only need to acknowledge the human soul is a possible 
metaphysics, and all the consequences thereof. It is a working class version of 
Pascal's Wager.

I would oppose this neoliberalization of the soul even if it were good science. But I 
double down against the Artificial Consciousness scientism crowd because I also think 
it is not good science. You cannot just whimsically redefine words to suit your 
science and think it will have no social impact on your social species.

Yes, I know there are socialists who truly believe it is more "honest" and "adult" 
to scientificize human consciousness. I think they're from another planet. There 
are no friends I feel more sorry for, and I have a fair few of them. (One tragedy is 
*they* feel sorry for *me*, or maybe they deny that they have subjective feelings? 
I'm not sure.) They clearly have at least not thought about the social consequences 
of their "scientific socialism". It is completely unscientific for starters, but a 
gross distortion of the human experience for another.

We do not gain any more as a species if we deny our spiritual nature. We may not 
ever understand this nature, the profane may abuse those of weak minds who get 
driven into anti-spiritual cults pretending to be spiritual 
(Scientology, Christian Nationalism, ISIS, that sort of thing), but if we fool 
ourselves into thinking science can understand the subjective, the spiritual, we are, 
I think, on a slow road to something more like fascism (essentialization of that 
which ordinary working people sometimes cling to when their human soul is denied) 
than we are on a road to enlightenment.


### Lab Lunacy Leaks

By now you can tell I've only used Sabina Hossenfelder as an avatar for nerd 
scienticism. This article is not an attack on her personally, as I said, 
I love her show and think she is a wonderful science communicator and educator, 
and a whole lo smarter than me.  A true working class person's scientist. I think 
that's about all I have to write on this whole topic for now, maybe with a few last 
nuances.

One should note the massive distinction (totally different meaning) of the word 
"subject" when referring to a lab experiment subject, which could be a lump of clay, 
and the meaning of "the subjective" (mental qualia). A thing can be a subject to the 
experimentalist --- something we probe and prod at, without being conscious. 

The subject in an experiment is nothing like the meaning of "the subjective" which 
refers to inner private mental experiences. To confuse these words and their 
different meanings might perhaps be a deception in only the English language (though 
I am not sure, but at least in English it is a deception).

There is perhaps only one type of phenomenon that is truly pure subjectivity, namely a 
conscious being. But I cannot say we will not discover other subjective states of 
existence at some time in the future. It just would be very weird if a phenomenon was 
subjective in aspect (had inner states that cannot be objectified) and yet not 
conscious. However, I will offer one possible candidate: the platonic concepts in 
mathematics, like the transfinite cardinals, or large cardinals, probably have 
properties that no one will ever be able to objectively identify. I do not know if 
"failure to be able to objectify" counts as "not being objective" though. Actually, 
strike that, I *do* know: it definitely does *not* count. It's a failure of the 
observer, not a property of the thing.

"Failure to be possible to be objectified," on the other hand is a possible property 
of a phenomenon. Such phenomena, if they occur, would be purely subjective in some 
aspect.

I suspect the postmodernists are making the mistake of sliding from "subject" (lab 
experiment) to "the subjective" (mental qualia), and thinking they've come up with 
some profound insight into the nature of consciousness. I feel sorry for them. It is 
like sliding from golf to lunch --- from sand wedge to sandwich; or like sliding from 
crime to music --- from *a* rap to a *rap*.

<table style="border-collapse: collapse; border=0;">
    <colgroup>
       <col span="1" style="width: 40%;">
       <col span="1" style="width: 30%;">
       <col span="1" style="width: 45%;">
    </colgroup>
<tr style="border: 1px solid color:#0f0f0f;">
<td style="border: 1px solid color:#0f0f0f;"><a href="../21_susskinistas">Previous chapter</a></td>
<td style="border: 1px solid color:#0f0f0f; text-align:center;"><a href="../">Back to Blog</a></td>
<td style="border: 1px solid color:#0f0f0f; text-align:right;"><a href="../23_pygeons">Next chapter</a></td>
</tr>
<tr style="border: 1px solid color:#0f0f0f;">
<td style="border: 1px solid color:#0f0f0f;"><a href="../21_susskinistas">Susskinistas</a></td>
<td style="border: 1px solid color:#0f0f0f; text-align:center;"><a href="../">TOC</a></td>
<td style="border: 1px solid color:#0f0f0f; text-align:right;"><a href="../23_pygeons">Computational Geons</a></td>
</tr>
</table>
