---
title: "Free Botherations"
date: 2023-03-15
toc_depth: 3
draft: false
katex: true
toc: true
disableTitleSeparator : true
---

All this distraction writing about AI and platonism is sucking my time away from 
macroeconomics and T4G, but I hope for posterity all these musing are useful to some 
kids who might get some inspiration. Today *The Algorithm* took me to 
[Machine Learning Street Talk](https://www.youtube.com/@MachineLearningStreetTalk) 
and an interesting 
[interview with Karl Friston](https://www.youtube.com/watch?v=V_VXOdf1NMw), an eminent 
neuroscientist who has some wacky ideas about cognition, relevant to AI and to the 
anti-platonistic schools of thought.

I hate to be combative, so I'll try to review Friston and the so-called 
*free-energy principle* fairly, but I'm an apologetic platonist (see 
[previous posts, 18](../19_plates_and_socks) and [19](../20_platonics)) 
and so will have to call out idiocy as regards blind materialist dogma when I see it. 
That's a trigger warning for materialists, so you've been fairly warned. If you want 
to debate me then please [donate](https://ko-fi.com/achrononmaster/) first, 
since I cannot live on github/substack views and dislikes alone. This way you are 
free to call me an idiot too, but not free to get my response. I may end up agreeing 
with you, but you'll have to pay to read that --- at least until such time as my 
government employs me to do something more fun filled.


## Free Bananas - TL;DR

The great thing about the internet and the various arxivs, is that a boat load of 
cool thinking is no longer locked behind journal pay-walls, and we get to see 
interviews with bright thinkers on youtube for free entry (for Zoomers who weren't around in the 80's you need not bother your brains with that comment). There is a 
real cost though, opportunity to do better things goes begging. With that in mind, I 
am a little torn about how much effort to spend writing about Friston's school of 
thought in cognitive science.

I will thus try to do a quick overview, before diving into specifics, and that way 
the tldr crowd can be on their way shortly.
This is my best current synopsis:

1. The free-energy principle is a mathematical identity obtained from physical 
assumptions about dynamical systems subject to entropy increase (so admitting 
statistical descriptions, or equivalently, any high level description ignoring a lot 
of molecular level and atomic degrees of freedom).
2. As a mathematical identity, the FEP says nothing useful about anything unless there 
is a system satisfying the prerequisites of the theorem.
3. The use of this principle in cognitive science is a simple tautology, that observes 
the outcome of any adaptive evolution (assuming some are energy through-put 
organisms) will be organisms which are in a rough sense optimized to minimise a 
quantity called "surprise" (although this does not mean phenomenal conscious 
surprise, it is a dopey use of a *thick concept* in what is really a very narrow 
range, and it can be computed from basic information theory estimations.)
4. It is possible to form normative models (so no black swans, like extreme altruism 
or insanity) of cognitive behaviour by supposing organisms "cognate" based on 
using sensory input to determine actions the organism can take in order to keep the 
"surprise" of their environment low.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **(a)** The terms "cognate" and "surprise" are 
loaded terms, the latter having been already loosely defined using information 
theory, the former entirely undefined, and considered for our purposes to be some 
undefined generic sort of computation, although the nature of which is unclear, in 
animals it takes place mostly in the brain rather than individual cells.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **(b)** There is no theory in FEP related 
literature covering anything like subjective mental qualia. Qualia are not in the 
realm of FEP applications in cognitive science, so FEP stuff is all purely 
behavioural. No semantics.
5. FEP applies to single-celled organisms and plants, and things without brains. 
Which tells you the "cognition" we are talking about is probably not consciousness, 
not by a long stretch (unless you believe in panpsychism). So there is no explanation 
for subjective consciousness in any of this program, so it has nothing to do with 
true genuine "AI" (artificial consciousness, or sentient programs or sentient robots).
6. While FEP is a theorem, the application in any given model is contingent. 
So the FEP models can be falsified, simply by finding organisms that display your 
*generalized* cognitive capacity but which do not satisfy the FEP assumptions; or by 
finding predictions of the model that are not born out by facts. 


## A Deeper Dive into the Banana Pool

First, if you want a view on AI quite similar to mine, listen to 
[Mark Bishop](https://www.youtube.com/watch?v=e1M41otUtNg). 
FWIW I do not think much of the argument derived from Putnam, since a hard-nosed 
materialist can always adopt mysticism and claim some sort of panpsychism generates 
conscious qualia in *any and all* physical computations. This is similar to John 
Searle's ideas, except Searle mysterianism restricts qualia generation to 
hypothetically only certain special biological processes (for lord knows what reason, 
he gives no convincing reason other than we know humans are biological organisms).

The counter-Putnam-Bishop argument is that all computations with the same states are 
not equal. It matters how a process goes through the physical states. No one claims 
to know what the special types of process are that purportedly generate conscious 
qualia, it is simply taken on faith, by materialists. They have to take it on faith, 
because there is no other option for them, except to accept Putnam's *reductio ad 
absurdem* as valid which thus ridicules the whole idea of a computational basis for 
consciousness.

The more extended Church-Turing thesis is thus that not all computations generate 
qualia, only a special type, and the comptutional sequence of states is not the 
generator, the generator of qualia is something else, "yet to be determined." 
Presumably this something else involves at a minimum the time between states in the 
computer registry, and for the ‘quantum consciousness’ quacks${}^\dagger$, presumably 
also a certain amount of entanglement.

${}^\dagger$I do not mean to belittle crazy ideas, and it is not that I think 
consciousness is unrelated to a physics which is quantum mechanical, I think 
consciousness definitely only manifests in universes with something *like* quantum 
mechanics. What I ridicule are people who think they know how quantum mechanics 
accounts for consciousness. Those people are completely deluded. The correct way (I 
think) to think about the relationship is that consciousness cannot get manifested in 
a universe which lacks something akin to quantum dynamics, but the quantum mechanics 
is not the generator of the qualia, no more than classical mechanics can be the 
generator.

Supposing any and all computations generate qualia is not the claim here, the claim 
is only that *physical* computations (any and all of them) generate qualia. No one 
ever says how though, so it is total bullsh*t (until proven otherwise). 

> Good science is the opposite of the law, you are guilty until 
proven innocent, and no science is ever proven innocent.

### On those who think the C Compiler understands C Language

Walid Saba, normally a reasonable guy, made an absurd claim in one of the MLST 
[comments sections](https://www.youtube.com/watch?v=e1M41otUtNg), 
he claimed machine "understanding" is possible, he cited the Python language 
compiler as an example: it must understand the Python language, he 
suggested. Here was my reply:

> Bollocks mate. The python compiler does what van Rossum (et al) told it to do, and 
what subsequent users command. The understanding is all derivative from the coders. 
You cannot mathematicize mental qualia because they're not mathematical things.  But 
if you choose to define "understanding" in black-box operation terms, then of course 
you can program a machine to "understand." But it'll never be generating subjective 
mental qualia, so cannot understand in the sense I would define it.    
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There is no test for such subjective inner 
sentience either, that's Turing's point. And Nagel's. It is an imitation game, not a 
mental qualia generation game.  So if perchance someone grows a computer that gains 
subjective mental qualia generative capacity, no one will ever know. It's not a 
falsifiable hypothesis.

Also, there is no "It" that is "the python compiler." So there's no conscious 
subject to speak of. In black-box input-output terms, you could identify an 
operational capacity of "understanding" in almost anything. Which makes it almost 
meaningless. Better to stick to what most normal people think of when they say, 
"I understand." It is not a mathematically definable attribute.

My colleague, Douglas the MMT Macro Trader, would here object (I know, because he 
has) that all humans are doing is derivative from past human activity too, so doesn't 
my argument apply to us as well?

My answer is, well maybe for *you* yes. But not for me.

I know I experience mental qualae. I do not know for sure you all do. But I can 
legitimately infer you also experience qualia, as a gesture of humanity, the 
Silver Rule (I would not wish you to assume I am not conscious either).

Why do I not extend the same gesture to a machine that passes all imitation games?

Well, you see, I might.  But I cannot extend the gesture to a piece of software, nor 
the machine it is running on, I can only extend the gesture of inferring the thing is 
conscious to a system as a whole. But then, I have to be highly sceptical, because of 
the Nagel problem: *what is it like to be a bat?*
I have no frickin' idea, and so no clue about whether bats have minds or are not just 
zombies with sophisticated behaviours. 

When I say "I don't have a clue" I also mean the entire body of the science 
community has no clue, I'm not making a provincial argument here.

At the very least, I want an entity to be able to do a bit of symbolic algebra, 
and not have been programmed to do so, but learned through social interactions. I also
rather fancy I'd like to see the entity spontaneously freak out like the blue blazes 
if I go anywhere near it's ‘off’ switch. Or whatever the equivalent for a machine is 
of moving a hot poker near a monkey's private parts.

If a Bat or a Monkey can do all this, I will morally be obliged to treat it as 
conscious, a non-zombie. Holding out the possibility other animals might have such 
hidden capacities, I will treat them with kindness anyway. But I have no care about 
the consciousness of a lump of clay, I will tread on that stuff all day long without 
any moral qualms. I realize this may seem all gross, selfish, and disgusting for 
people who think molecules have feelings, so, ok, I am sorry.

I do know one human being experiences mental qualia, and so can infer others do too, 
but there is no evidence subjective mental qualia arise from brains, so the inference 
to machines and other animals cannot be made. It would be a complete leap of faith, 
so unscientific. Perhaps morally justifiable, but not scientifically.

What we do know is that our conscious faculty needs a brain in order to exert 
physical causal efficacy. No one has any idea what this causal power is, where it 
comes from, or anything of that nature. Once you try to figure it out all that 
happens is you go down a reductionist tunnel to the elementary particle gauge forces, 
and so remove all notions of the subjective qualia you were trying to account for --- 
it's the grandest "hand-in-the-cookie-jar problem" of all time. The only way to get 
the cookie out is either by magic (illusion) or to crumble it, destroying the 
cookieness (except for purposes of the Ben & Jerry flavour) 

I think Mark Bishop needs some nuance though. The reason we cannot be living in a 
computer simulation is not because, "I can feel the sensation of cool water on my 
face in a rain shower." That sensational qualia is the reason we cannot be living in a 
*physical* computation. But if you extend the entire meaning of "computation" to 
something very metaphysical, like, "thought processes in the Mind of God" then 
anything is possible, up to logic. No one is going to do this in the AI geek 
community though, let's be honest.

Instead we define a "computation" as that which can be simulated on an abstract 
Turing machine (so we need unphysical infinite tape for starters, but most 
philosophers are prepared to grant materialists this loophole. I am.) I doubt having 
an infinite tape is going to endow a system with subjective conscious qualae.

This is where it gets philosophically interesting. If physical computations cannot 
generate subjective mental qualia, then what can?

I appreciate all materialists clearly think either mental qualia are illusions 
(illusions to *what* or *whom* exactly, if not a mentally aware entity?) or that 
physical computations (generalized physical processes) *can* generate mental qualia. 
So they are not going to accept the definition of mental qualia being subjective, 
they are committed to objectification. That's completely banana philosophy, but I'm 
unable to shift their views, except with ridicule, which is ineffective, so will not 
waste further effort trying.


```
TODO: this is an article stub.
More to come later.
```

## "I Am Therefore I Think" --- and also you're insane

Later I should watch the talk, but Friston has this lecture with that title, minus 
the insanity clause. I'll swallow the clickbait and do the idiot thing in only 
commenting on the title! It's ok to use a clickbait title to rail against insanity 
even if the talk beneath the title is sound and moderate, provided that's what you 
declare to be doing, constructing a strawman. That's what I am doing.

OK, so to be brief, "I Am therefore I Think" is a massive case of warped logic. 
Sounds like panpsychism, poorly motivated. However, there is an easy way to
reconcile it with decency.

Any entity that knows it exists, and can contemplate "I Am" will *ipso facto* 
know that it thinks. Before it even gains a social construction of the verb "to think." 

There are plenty of things I have no language for, but which exist. People will say 
that I then have a mental language, but that's a lot of bullsh$\ast$t. Our mental 
qualia are not a language. However, if you are Jerry Fodor or someone with a galaxy 
brain, for sure you can redefine the meaning of "language" and claim mental paint is 
a form of inner "language of thought." To my mind this gets us nowhere, because 
without grist backing your definition (what is *the mental*) your definition has no 
practical use. It becomes just academic philosophy word gaming. 

So to rescue Friston's insanity we change the statement to a true one:

> I have the knowledge "I Am," therefore I think.

Which is sensible, and is just restating Descartes in a subtler way. It is a partial 
definition of what subjective knowledge connotes.  It connotes subjective awareness, 
so there is a conscious entity, an "I". If a thing merely exists, it does not 
necessarily have any "Am-ness" about it, so cannot even think to think it is 
thinking.

But surely Friston cannot be so lame?  There must, methinks, be something else he is 
getting at by inverting Descartes.  What is it?

After all, the inference, 
$$
(\text{I Think}) \stackrel{?}{\Rightarrow} (\text{I Know I exist})
$$
is unidirectional. If there is no conscious "I" then we cannot go in the reverse 
direction. However, the above implication is incomplete, for the above reason. One 
must not only be thinking, one must have the thought of oneself as a being. Then the 
implication becomes a tautology. 

I am going to play my guessing game to motivate reading or listening more to Friston. 
Without the game I could not tolerate the tedium. For the set-up I am going to 
conjecture what Friston might be talking about.

### What Might Friston Mean?

Could it be that Friston means nothing more than the banality that any system which 
is self-aware must also be thinking? I think that is what he'll be saying, using a 
few thousand superfluous words.

However, from what I can tell after studying the ideas a bit there is really nothing 
in the FEP/Active-Inference paradigm that relates to subjective consciousness, so I 
rather quickly tired of the work, because it ends up just being another useful tool 
for studying and modelling complex behaviour. Like neural networks. 

The distinguishing feature f all such tools is that they escape the need to write 
down physics governing equations. I you need to discover a set of PDE's for every damn 
complex system you are going to be intellectually paralysed. Instead applied complex 
system scientists have invented all sorts of models for avoiding the need to write 
down governing PDE or ODE equations. 

These range from cellular automata, agent-based systems, neural nets, 
Bayesian nets, expert-systems, GOFAI symbolics, and many others I've long since 
forgotten.

The trick is, you want to use a few hundred thousand words to confuse your audience, 
to hide the banality.  The banality being that he will be using the concept of 
"self-awareness" as a proxy for thinking, without defining precisely what 
"self-awareness" connotes because no one knows what it connotes, since no one knows 
what the hell consciousness is in essence.

Thus, I conjecture, Friston is saying nothing much more than:

<div style="text-align: center; font-style: italic;">
A self-aware system is self-ware.</div>

But he will do so in so many more words such that everyone will think he is saying 
something profound. This is why academics get paid the big bucks.

When we professionalize thinking we degrade our thought. (Not always ok, there are 
some good folks. Also, just because I am not paid to think does not make my writing 
more profound. I shouldn't have to point this out, but you are never free of the 
pedants, so a little effort to calm them down 
\[or do I mean clam them down?\] might not go astray.)

I am also going to hedge a bit though.

I am pretty sure the FEP/AcIn ideas will be usefully pplied to what they call "machine learning" but which I call *behavioural modelling*.  I think the FEP/AcIn stuff will have useful insights for behavioural modelling.

It should also have useful insights for cognitive science, because the input--output aspects of cognition are a special type of behaviour.But I suspect that's about the end of it. A useful modelling paradigm. Like the Euler equations are a useful model for some fluid lows.

To write further I guess I'll have to listen Friston's tedious lecturing. Maybe 
tomorrow I'll write an update.

```
TODO: what was Friston thinking?
```


```
TODO: had some other things to include...
```

## Is the Free-Energy Principle school any great use?

Yes and no. 

It's of no use for gaining insights into human subjective consciousness, because the 
FEP/AcIn school is descriptive, and purely mathematical. If conscious deliberation 
conforms to FEP constraints that is not because of the FEP dictating anything, it is 
only because whatever a conscious being happens to be, their body + brain dynamics is 
temporarily satisfying the presumptions of the FEP theorem. 

I can use a quantum physics analogy or metaphor. Suppose Bohm Pilot waves are the 
reality. Does this mean Many worlds is useless? No. I can still use Many Worlds 
Theory to make predictions, perfectly in accord with reality. 

Whatever a conscious organism is doing when "thinking" it is not implementing a 
free-energy minimisation procedure, not at all. It's dynamics are historically 
dependent, and black swans can occur all over the show that grossly violate the FEP 
demands. We operate under such non-FEP all the time. But on average we probably 
satisfy the FEP constraints. So it is not the FEP that is driving conscious beings. 
But the FEP/AcIn does a good job nonetheless in describing certain normative aspects 
of conscious deliberation.

So like a Many Worlds QM is *useless for metaphysics* if in fact reality is Bohmian 
(or think the converse if you're a Many Worlder) the FEP/AcIn paradigm is useless for 
understanding consciousness. However, FEP/AcIn could be terrific for designing 
automated systems for specialist tasks. Just as Many Worlds might be very useful for 
designing quantum circuits (I actually doubt that is true, but more than few quantum 
computer engineers are Many Worlders).

((I might be being unfair to Friston's school here, maybe they never claimed 
FEP/AcIn is research aimed at understanding consciousness?))

Ultimately, what I think is going on under the FEP "Active Inference" banner of 
thought is something descriptive. This turns out to be useful in designing 
"intelligent" systems (automated intelligence, not genuine intelligence).

The way I think things work in the real world is that consciousness is pretty darn 
unique, and can be goal-directed, or teleological, but that implies some 
as-yet-unknown top-down causation. The FEP and Active Inference do not show us how 
top-down causation can occur. This means FEP/AcIn is a research paradigm for getting 
better automation. It is not a possible research program for understanding human 
types of sentience (subjective consciousness).

A conscious being can thus work outside the parameters of FEP/AcIn, and we see this 
all the time in what I'd call "black swans" in real world societies, altruism and 
cooperation extending beyond minimising free-energy, or for that matter insanity, war
and violence going beyond the FEP in the other direction.

Thus, if you wish to design an automated system with a mimicry for "intelligence" 
(which is nothing but a synonym here for sophisticated behaviour) then the FEP/AcIn 
paradigm is a good way to go about designing such systems.

Why does it work?

It works because most systems we would employ FEP/AcIn for use at particular tasks 
are going to be working in controlled environments. They are going to be operating 
under Gaussian probability distribution assumptions, unless explicitly programmed not 
to, which might be considered how you specialize. A machine system would specialize 
in the broadest sense by having non-Gaussian models for particular narrow tasks. 

But put them in front of a luddite with some C4 (for example, or anything you did not 
train your automated system to encounter) and I think your FEP/AcIn robot is going to 
experience a sudden black swan where it's FEP optimization little engine gets 
catastrophically rendered completely useless. 

Conscious beings can suffer from similar catastrophe of course, but we are far more 
robust against such black swans, because we're conscious. This has nothing to do with 
FEP. The black swan is beyond any FEP/AcIn rescuing.

It is a feature of complex non-linear dynamical systems that specialization comes at 
a cost: one cannot be a generalist. The entire history of human civilization is, in 
part, a story of our ability to use technology and science to get specialist systems 
to do what no human can ever do, even right down to mass manufacturing paper clips. 
This has prolonged human civilization, it has allowed us to all remain, for the most 
part, generalists. Thus without any further speciation.

The highly specialized human is a basket-case of ineptitude. Highly unfit for 
survival, unless protected by a caring society.


<table style="border-collapse: collapse; border=0;">
    <colgroup>
       <col span="1" style="width: 35%;">
       <col span="1" style="width: 25%;">
       <col span="1" style="width: 35%;">
    </colgroup>
<tr style="border: 1px solid color:#0f0f0f;">
<td style="border: 1px solid color:#0f0f0f;"><a href="../23_pygeons">Previous chapter</a></td>
<td style="border: 1px solid color:#0f0f0f; text-align:center;"><a href="../">Back to Philosophy</a></td>
<td style="border: 1px solid color:#0f0f0f; text-align:right;"><a href="../25_formal_models">Next chapter</a></td>
</tr>
<tr style="border: 1px solid color:#0f0f0f;">
<td style="border: 1px solid color:#0f0f0f;"><a href="../23_pygeons">Computational Geons</a></td>
<td style="border: 1px solid color:#0f0f0f; text-align:center;"><a href="../">TOC</a></td>
<td style="border: 1px solid color:#0f0f0f; text-align:right;"><a href="../25_formal_models">Formal Models</a></td>
</tr>
</table>
